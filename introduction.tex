As the gap between the available compute power and the cost of data
movement increases, data transfer, whether from caches, main memory,
or disk, is becoming the major bottleneck of many workflows. However,
it is well known that often not every byte of data is necessary to
answer scientific questions at sufficient accuracy.  In particular,
for techniques at the end of scientific workflow, like visualization
and data analysis, much coarser approximations of the data often
provide adequate
approximations~\cite{woodring2011,covra2012,compression_sim2013}, and
even during simulation some loss in precision is often
acceptable~\cite{compression_sim2013}. As a result, a host of
different techniques have been proposed to reduce the size of
data. Generically, these can be classified into approaches that reduce
the data resolution, i.e., the number of data points or coefficients
stored, and techniques that reduce the precision of each data point or
coefficient.  Examples of the former are adaptive mesh
refinement~\cite{amr1989}, octrees or other tree
structures~\cite{hierarchical1984} and of the latter
compression~\cite{zfp2014} or quantization~\cite{vq1992}.
Traditionally, multi-resolution structures have been used to
accelerate dynamic queries, for example, in
rendering~\cite{multires_octree1999}, since discarding data points
based on the view point or data complexity can result in significant
speed-ups. Compression based on uniform quantization on the other hand
is more common when storing data, where in the absence of other
information treating each sample as equally important is the
null-hypothesis. However, in many situations a combination of both
resolution and precision reduction would be appropriate.  For example,
we may need a high spatial resolution to resolve
the topology of an isosurface, yet we likely do not need the
corresponding samples at full precision to adequately approximate the geometry.
Similarly, we may need high precision values to accumulate accurate
statistics, yet a small subsample of points may be sufficient to do
so.  \note{Are we making this up, or do we have an example where such
  high precision is needed? PTB: We are totally making this up!}  

In general, there exists a wide range of potential analysis and
visualization task all of which may prefer different combinations of
resolution and precision and for many these requirements will be
data-dependent.  Consequently, we do not expect any particular data
organization to be ideal. Instead, we consider a progressive pipeline
in which some initial data is loaded and processed and new data is
requested selectively based on the requirement of the current task and
the characteristics of the data already loaded. However, while there
are almost certainly advantages in adapting both resolution and
precision, it is unclear how big the potential gains could be and how
much of the benefits can be accessed with practical algorithms.


%
% PTB: This does not seem to fit into the current argument chain
% anymore
%
% Some multi-resolution file formats like JPEG2000~\cite{jpeg2001} do
% provide per-block progressive decompression and thus allow at least
% some flexibility in adjusting both resolution and precision
% simultaneously. But we are not aware of any analysis pipeline that
% uses these capabilities beyond simple region-of-interest based
% queries.


Using a wide range of data sets and progressive instances of different
tasks, we show that selectively adjusting both resolution and
precision can provide significant advantages over adjusting only one
of them. In particular, compared to traditional techniques that either
adjust resolution according to some spatial ordering, i.e.\ an octree,
or adjust precision by processing data one bit-plane at a time, we
consider arbitrarily reordered streams of bits (or small chunk of bits)
optimized to reduce the resulting error.  We show that reordered
streams can not only provide---sometimes drastically---improved
results, but that different types of analysis require substantially
different bitstreams. This result implies that there exists no
universally good ordering to replace the multi-resolution or precision
based streams used today. Instead, an ideal system should support
adaptive and dynamic changes in the bit-ordering depending on the
task and the data.  While creating such a system is beyond the scope
of this paper, we provide: an offline optimization algorithm as a
stand-in for the ideal bitstream; and an online heuristic that
provides a good approximation to the theoretical ideal. The former
provides a crucial metric to evaluate the latter, since the the
theoretically optimal stream is NP-hard to construct and thus not
accessible for any data of relevant size. Our contributions is detail
are:

\begin{itemize}

\item An optimization algorithm that approximates the ideal bitstream
  for a given data-task combination ;
 
\item Using a suite of data sets and analysis tasks, we demonstrate
  that jointly optimizing both resolution and precision provides
  significant data reductions above and beyond  adjusting each
  separately;

\item A practical heuristic to dynamically create high-quality
  bitstreams for different analysis tasks.

\end{itemize}

 



% In recent years, compute bandwidth has been greatly outpacing the
% latency of data movement across all levels of the memory hierarchy,
% making data movement the bottleneck in almost all workflows that
% involve data creation or processing. When it comes to analyzing the
% terabytes of data produced by massively parallel simulation, this
% bottleneck prevents scientists from studying the data to gain
% scientific insights. Several data reduction techniques have been
% proposed to reduce the amount of data for storage and analysis, and
% they mainly fall into two categories: techniques that reduce data's
% resolution (e.g., octrees, wavelets), and techniques that reduce
% data's precision (e.g., quantization and truncation). As we will show
% in this paper, however, reducing data only in resolution or only in
% precision is not flexible enough. Much larger gain can be achieved by
% combining these two dimensions of data reductions.

% There exists data reduction techniques that combine resolution and precision, of which perhaps the
% most well known is JPEG2000. The specifications for JPEG2000 however do not handle high-precision 3D
% data commonly found in scientific applications. But more importantly, JPEG2000 data streams do not
% adapt to the analysis task at hand. We will show in this paper that a data reduction scheme should
% be task-dependent to maximize the reducibility potential, as different analysis tasks have different
% preferences in terms of the resolution and precision of data. These preferences possibly also change
% across the whole domain. It is therefore important to understand the characteristics of the analysis
% tasks to avoid reading and transmitting unimportant data bits.

% For a wide range of data sets, we construct different bit streams, each optimized for a common
% analysis task, and study their characteristics. The study presented in this paper is a step toward a
% data representation that allows bits pertaining to a given analysis task to be read efficiently,
% with minimal amount of unnecessary bits. Such a data representation would enable interactive
% exploration as well as cursory analysis of enormous, possibly remote data sets, without waiting for
% days of data transmission before figuring out what regions of the data to focus on.

% Our contributions are, in detail:

% \begin{itemize}
%   \item For fundamental analysis tasks, namely data reconstruction, gradient and Laplacian computation, iso-contour extraction, and histogram computation, we introduce a framework in which spatially adaptive data streams in both resolution and precision can be formed and compared. We also devise an optimization algorithm that approximates the optimal bit stream for each task. These streams serve as both empirical limits and sources of insights to designing heuristics to task-dependent .
%   \item Based on observations on the aforementioned task-optimized streams, we propose practical heuristics for:
%   \begin{itemize}
%     \item Reconstruction the function data accurately in the L2 norm, as well as its first and second derivatives (gradient and Laplacian).
%     \item Extraction of an iso-contour given the iso-value, by localizing the contour spatially, and applying the heuristic for minimizing the L2 norm of function error locally.
%     \item Computation of histograms, using a novel piece of ``metadata'' information called \emph{stream signature}.
%   \end{itemize}
% \end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "EGauthorGuidelines-eurovis18-full"
%%% End:
