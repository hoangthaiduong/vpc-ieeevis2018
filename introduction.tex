As the gap between the available compute power and the cost of data movement increases, data
transfer, whether from cache, main memory, or disk, is becoming the major bottleneck of many
workflows. However, it is well known that often not every bit of data is necessary to answer
scientific questions at required accuracy. In particular, for techniques at the end of scientific
workflows, such as visualization and data analysis, much coarser approximations of the data often
provide adequate approximations~\cite{woodring2011,covra2012,compression_sim2013}, and even during
simulation some loss in precision is often acceptable~\cite{compression_sim2013}. As a result, a
host of different techniques have been proposed to reduce the size of data. Generally, these
techniques can be classified into approaches that reduce the data resolution, i.e., the number of
data points stored, and techniques that reduce the precision of each data point.

Examples of the former are adaptive mesh refinement~\cite{amr1989}, octrees or other tree
structures~\cite{hierarchical1984}, and of the latter compression~\cite{zfp2014} or
quantization~\cite{vq1992}. Traditionally, multi-resolution structures have been used to accelerate
dynamic queries, for example, in rendering~\cite{multires_octree1999}, since discarding data points
based on the view point or data complexity can result in significant speed-ups. Compression based on
uniform quantization on the other hand is more common when storing data, where in the absence of
other information, treating each sample as equally important is the null-hypothesis. However, in
many situations a combination of both resolution and precision reduction could be appropriate. For
example, we may need a high spatial resolution to resolve the topology of an isosurface, yet it is unlikely we
need the corresponding data samples at full precision to adequately approximate the
geometry. Conversely, we may need high precision values to accumulate accurate statistics, yet a
lower-resolution subset of points may be sufficient to do so. 

In general, there exists a wide range of potential analysis and visualization tasks all of which may
prefer different combinations of resolution and precision, and for many, these requirements will be
data-dependent. Consequently, we do not expect any particular data organization to be ideal for all
cases. Instead, we consider a progressive setting in which some initial data is loaded and processed,
and new data is requested selectively based on the requirement of the current task and the
characteristics of the data already loaded. The result is a stream of bits that should minimize
the error of the task at hand. However, while intuitively there are almost certainly advantages in
adapting both resolution and precision, it is unclear how big the potential gains could be and how
much of the benefits can be realized with practical algorithms. This paper aims to answer these
important questions through extensive, empirical experiments. Our contributions are:

\begin{itemize}
\item We present an in-depth study of the resolution-versus-precision trade-offs for fundamental
  data analysis and visualization tasks. To make this study possible, we put these trade-offs in the
  framework of streaming wavelet coefficients, interleaving bits that improve precision and bits
  that improve resolution (Section \ref{sec:terminologies}). In this way, we can represent different
  data streams in a unified framework and perform fair comparisons.
   
\item We demonstrate how jointly optimizing resolution and precision provides significant
  improvements on the results of analysis tasks over adjusting resolution or precision independently
  (Section \ref{sec:motivation}). In particular, we provide empirical evidence using a collection of
  data sets and analysis tasks. We also show how different types of analysis require substantially
  different bit streams for optimal results.

\item We estimate lower bounds of error for various analysis tasks using a greedy aprroach that
  jointly optimizes resolution and precision (Section \ref{sec:data_dep_streams}). In addition, we
  also present heuristics to construct high-quality data streams closely approximating these bounds
  (Section \ref{sec:derivatives}, \ref{sec:histogram}, \ref{sec:isocontour}).
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "EGauthorGuidelines-eurovis18-full"
%%% End:
