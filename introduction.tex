As the gap between the available compute power and the cost of data movement increases, data
transfer, whether from cache, main memory, or disk, becomes a major bottleneck in many workflows.
However, it has been shown that not every bit of data is always necessary to answer scientific
questions with required accuracy. In particular, for techniques at the end of scientific workflows,
such as visualization and data analysis, lower fidelity representations of the data often provide
adequate approximations~\cite{woodring2011,covra2012,compression_sim2013}; even during
simulation, some loss in precision is often
acceptable~\cite{compression_sim2013,doi:10.1177/1094342018762036}. As a result, several different
techniques have been proposed to reduce the size of data. 

Broadly, these techniques either (i) reduce the data resolution, e.g., the number of data points
stored, or (ii) reduce the precision of each data point. Examples of the first kind of approach are
subsampling~\cite{idx2001}, adaptive mesh refinement~\cite{amr1989}, octrees and other tree
structures~\cite{hierarchical1984}, and wavelets~\cite{woodring2011}, and those of the second are
various forms of compression~\cite{fpzip,isabela,zfp2014,sz,vq1992,compression_domain2003,sqe}.
Traditionally, multiresolution structures have been used to accelerate dynamic queries, e.g., in
rendering~\cite{multires_octree1999}, since discarding data points based on the viewpoint or data
complexity can result in significant speed-up. Compression based on quantization, on the other hand,
is more common when storing data, where in the absence of other information, treating each sample as
equally important is the null hypothesis. However, in many situations, a combination of both
resolution and precision reduction may be appropriate. For example, high spatial resolution may be
needed to resolve the topology of an isosurface, yet the corresponding data samples may be usable at
less than full precision to adequately approximate the geometry. Conversely, accumulating accurate
statistics may require high-precision values, yet a lower resolution subset of data points may be
sufficient for the task.

In general, different levels of adaptivity in combinations of resolution and precision may be
suitable for different types of analysis and visualization tasks, and for many, these requirements
might be data dependent. Consequently, a globally optimal data organization may not exist. Instead,
we consider a progressive setting in which some initial data is loaded and processed, and new data
is requested selectively based on the requirements of the current task as well as the
characteristics of the data already loaded. The result is a stream of bits ordered such that the
error is minimized, considering the task at hand. However, although intuitively considering both
resolution and precision in the ordering certainly has advantages, it is unclear how much the error
could be reduced for a given data budget or how little data could be used to achieve the same error.
Furthermore, optimal data-dependent orderings may be especially impractical since they assume
perfect knowledge of the data. We therefore need to understand which of these potential gains are
realizable. This paper aims to address these problems about the suitable bit orderings through
extensive, empirical experiments. In particular, this paper presents:

\begin{itemize}\dense
%
\item a framework that allows systematic studies of the resolution-versus-precision trade-off for
common data analysis and visualization tasks. The core idea is to represent various data reduction
techniques as bit streams that progressively improve data quality in either resolution or precision
(\Cref{sec:terminologies}). We can thus compare these techniques fairly, by comparing the
corresponding bit streams.
%  
\item empirical evidence that jointly optimizing resolution and precision can provide significant
improvements on the results of analysis tasks over adjusting either independently. We present a
diverse collection of data sets and data analysis tasks and also show how different types of data
analysis might require substantially different data streams for optimal results.
%
\item a greedy approach that gives estimations for lower bounds of error for various analysis tasks.
In addition, we also identify practical streams that closely approximate these bounds for each task
(\Cref{sec:rmse-optimized}, \Cref{sec:derivatives}, \Cref{sec:histogram}, and \Cref{sec:isocontour})
using a novel concept called \emph{stream signature} (\Cref{sec:stream-signature}), which is a small
matrix that captures the essence of how a bit stream navigates the precision-versus-resolution
space.
\end{itemize}

In the third contribution, we consider the following analysis tasks: function
reconstruction, derivative computation, histogram computation, and isosurface extraction. The focus
is on common tasks, with a notable omission of volume rendering. Many parameters affect the quality
of a rendered image (e.g., transfer function, camera position, resolution of the image, and
rendering method), making it less tractable to design a scientifically sound study with
generalizable results.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "template"
%%% End:
