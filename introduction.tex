As the gap between the available compute power and the cost of data movement
increases, data transfer, whether from cache, main memory, or from disk,
becomes a major bottleneck in many workflows. However, it has been shown that
not every bit of data is always necessary to answer scientific questions with
required accuracy. In particular, for techniques at the end of scientific
workflows, such as visualization and data analysis, lower fidelity
representations of the data often provide adequate
approximations~\cite{woodring2011,covra2012,compression_sim2013}, and even
during simulation some loss in precision is often
acceptable~\cite{compression_sim2013,doi:10.1177/1094342018762036}.  As a
result, a variety of different techniques have been proposed to reduce the size
of data. 

Broadly, these techniques can be classified into (i) reducing the data
resolution, e.g., the number of data points stored, and (ii) reducing the
precision of each data point.  Examples of the former kind of approaches are
adaptive mesh refinement~\cite{amr1989}, octrees or other tree
structures~\cite{hierarchical1984}, \peter{What about subsampling, as in IDX?
And what about wavelets and other multires structures?}, and those of the
latter are compression~\cite{fpzip,isabela,zfp2014,sz} or
quantization~\cite{vq1992,compression_domain2003,sqe}. \hb{missing reference} Traditionally,
multi-resolution structures have been used to accelerate dynamic queries, e.g.,
in rendering~\cite{multires_octree1999}, since discarding data points based on
the view point or data complexity can result in significant speed-up.
Compression based on uniform quantization, on the other hand, is more common
when storing data, where in the absence of other information, treating each
sample as equally important is the null-hypothesis. However, in many
situations, a combination of both resolution and precision reduction could be
appropriate.  For example, high spatial resolution may be needed to resolve the
topology of an isosurface, yet the corresponding data samples may be usable at
less than full precision to adequately approximate the geometry.  Conversely,
accumulating accurate statistics may require high precision values, yet
lower-resolution subset of points may be sufficient for the task. 

In general, all types of analysis and visualization tasks may prefer different
combinations of \hb{replace `combinations of' with `levels of adaptivity in'?}
resolution and precision, and for many, these requirements will be
data-dependent. Consequently, an ideal-for-all data organization may not exist. 
%we do not expect any particular data organization to be ideal for all cases. 
Instead, we consider a progressive setting in which some initial data is loaded
and processed, and new data is requested selectively based on the requirement
of the current task and the characteristics of the data already loaded. The
result is a stream of bits that minimizes the error in the task at hand.
However, although intuitively there are almost certainly advantages in adapting
both resolution and precision, it is unclear how big the potential gains could
be \hb{the previous sentence is not convincing} \peter{The gains of what,
specifically? An ``optimal'' ordering?} and how much of the benefits can be
realized in practice.  This paper aims to answer these important questions
through extensive, empirical experiments. In particular, our contributions are
listed below.

\begin{itemize}
%
\item We present a framework that allows systematic studies of the
resolution-versus-precision tradeoffs for fundamental data analysis and
visualization tasks. The core idea is to represent various data reduction
techniques as data streams that improve data quality in either resolution or
precision in each step (\autoref{sec:terminologies}). We can
thus compare these techniques fairly, by comparing the corresponding data
streams.
%  
\item We provide empirical evidence that jointly optimizing resolution and
precision can provide significant improvements on the results of analysis tasks
over adjusting either independently.  This claim is demonstrated using a
collection of data sets and data analysis tasks. We also show how different
types of data analysis might require substantially different data streams for
optimal results.
%
\item We estimate lower bounds of error \peter{You either estimate the error or
provide a bound---not both.} for various analysis tasks using a greedy approach
that jointly optimizes resolution and precision (\autoref{sec:data_dep_streams}).
\peter{Too much going on here.  An error bound, a greedy algorithm, joint
``optimization'' of precision and resolution---whatever that might mean.  How
are these all related?} In addition, we also identify practical streams that
closely approximate these bounds for each task (\autoref{sec:rmse-optimized},
\autoref{sec:derivatives}, \autoref{sec:histogram}, and \autoref{sec:isocontour}), using
a novel concept called \emph{stream signature} (\autoref{sec:stream-signature}).
\peter{Add one sentence that gives a concise description of what a ``stream
signature'' is.}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "EGauthorGuidelines-eurovis18-full"
%%% End:
