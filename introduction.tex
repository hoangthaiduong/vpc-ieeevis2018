As the gap between the available compute power and the cost of data movement increases, data
transfer, whether from cache, main memory, or from disk, becomes a major bottleneck in many
workflows. However, it has been shown that not every bit of data is always necessary to answer
scientific questions with required accuracy. In particular, for techniques at the end of scientific
workflows, such as visualization and data analysis, lower fidelity representations of the data often
provide adequate approximations~\cite{woodring2011,covra2012,compression_sim2013}, and even during
simulation some loss in precision is often
acceptable~\cite{compression_sim2013,doi:10.1177/1094342018762036}. As a result, a several different
techniques have been proposed to reduce the size of data. 

Broadly, these techniques can be classified into (i) reducing the data resolution, e.g., the number
of data points stored, and (ii) reducing the precision of each data point. Examples of the former
kind of approaches are subsampling~\cite{idx2001}, adaptive mesh refinement~\cite{amr1989}, octrees
and other tree structures~\cite{hierarchical1984}, and wavelets~\cite{woodring2011}, and those of
the latter are various forms of
compression~\cite{fpzip,isabela,zfp2014,sz,vq1992,compression_domain2003,sqe}. Traditionally,
multiresolution structures have been used to accelerate dynamic queries, e.g., in
rendering~\cite{multires_octree1999}, since discarding data points based on the viewpoint or data
complexity can result in significant speed-up. Compression based on quantization, on the other hand,
is more common when storing data, where in the absence of other information, treating each sample as
equally important is the null hypothesis. However, in many situations, a combination of both
resolution and precision reduction could be appropriate. For example, high spatial resolution may be
needed to resolve the topology of an isosurface, yet the corresponding data samples may be usable at
less than full precision to adequately approximate the geometry. Conversely, accumulating accurate
statistics may require high-precision values, yet a lower resolution subset of data points may be
sufficient for the task. 

In general, different levels of adaptivity in combinations of resolution and precision may be
suitable for different types of analysis and visualization tasks, and for many, these requirements
will be data dependent. Consequently, a globally optimal data organization may not exist. Instead,
we consider a progressive setting in which some initial data is loaded and processed, and new data
is requested selectively based on the requirement of the current task as well as the characteristics
of the data already loaded. The result is a stream of bits ordered such that the error is minimized,
considering the task at hand. However, although intuitively there are almost certainly advantages in
considering both resolution and precision in the ordering, it is unclear how much the error could be
reduced for a given data budget or how little data could be used to achieve the same error.
Furthermore, optimal data dependent orderings especially may not be practical since they assume
perfect knowledge of the data. It is therefore important to understand which of these potential
gains are realizable. This paper aims to answer these important questions through extensive,
empirical experiments. In particular, our contributions are:

\begin{itemize}
%
\item A framework that allows systematic studies of the resolution-versus-precision tradeoffs for
common data analysis and visualization tasks. The core idea is to represent various data reduction
techniques as bit streams that improve data quality in either resolution or precision in each step
(\Cref{sec:terminologies}). We can thus compare these techniques fairly, by comparing the
corresponding data streams.
%  
\item Empirical evidence that jointly optimizing resolution and precision can provide significant
improvements on the results of analysis tasks over adjusting either independently. This claim is
demonstrated using a diverse collection of data sets and data analysis tasks. We also show how
different types of data analysis might require substantially different data streams for optimal
results.
%
\item A greedy approach that gives estimations for lower bounds of error for various analysis tasks.
In addition, we also identify practical streams that closely approximate these bounds for each task
(\Cref{sec:rmse-optimized}, \Cref{sec:derivatives}, \Cref{sec:histogram}, and
\Cref{sec:isocontour}), using a novel concept called \emph{stream signature}
(\Cref{sec:stream-signature}), which is a small matrix that captures the essence of how a bit stream
navigates the precision-versus-resolution space.
\end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "template"
%%% End:
