\section{Discussion}

We present the first empirical study to demonstrate that combining precision and resolution can lead
to significant gain in reduction, and that different tasks might prefer different
resolution-versus-precision tradeoffs. For example, while histograms require high precision,
computing derivatives benefits more from higher resolution. For brevity we focus on on a small set
of core tasks with a single metric for each task. However, the framework is generic and applies to
any well-defined metric and one future direction is to consider a broader set of domain-specific
tasks. We did not address volume rendering, where the results will heavily depend on the viewpoint
and transfer function. However, our experience has been that in practice, many volume rendered
images simply show layers of translucent isosurfaces. For these cases, our results on isosurfaces
provide a good stand-in.

An important question is whether task- and data-dependent streams provide sufficient advantages
compared to purely static ones. In practice, data would be used for multiple, not-necessarily
predefined tasks and maintaining multiple streams will likely lead to additional overheads. Here, we
consider \sopt to be the ideal data- and task-dependent stream and \ssig to be the best possible
data-independent stream. In both case some improvements are likely possible, e.g., a better
optimization for \sopt and different ideas for creating \ssig, but one would not expect these to
lead to fundamentally different results. Given these assumptions and the fact that \ssig in most
cases provides very similar results to \sbit or \swav, the additional effort (and potential
overheads) for task-dependent orderings it unlikely to be beneficial. This leaves a significant gap
between the good static streams, in particular \swav and \sbit, and the optimal stream \sopt. Our
experiments suggest that the majority of this difference can be attributed to spatial adaptivity.
The prototypical example are iso-surfaces where \sopt can skip all regions that do not effect any
portion of the surface.

Ultimately, our results can guide development of new data layouts and file formats for scientific
data. It seems \swav provides the best all around performance especially if it could be coupled with
a spatially adaptive hierarchy. However, for a file format additional constraints must be
considered, such as disk block sizes, cache coherence, and compression. Furthermore, an ideal format
should allow task-dependent data queries even though task-dependent formats will likely be
restricted to very specific situations. 

Another important consideration is that to provide a fair comparison we always reconstruct the data
at full resolution using the appropriate basis functions. This isolates the question of how much
information the data contains from how it is represented. However, in practice, processing and
memory cost are important factors, and given highly adaptive streams it is likely that adaptive
representations would be used~\cite{gigavoxels,Gobbetti2008,vdb2013}. In these cases the error of a
given approximation depends not only on the information that is available, but also on what
algorithm and what interpolation are used. For example, trilinear interpolation on a coarse grid
might provide vastly different results than wavelet reconstruction on the original mesh. It is
possible to specifically construct grids where both interpolations are equivalent~\cite{weiss}, yet
these are not yet implement in standard tools. An important future research direction will be to
understand the implications of the results presented here on existing tool chains, i.e.\ VTK.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "template"
%%% End:
