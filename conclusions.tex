\section{Discussion and Future Work}

This work addresses one of the biggest contemporary challenges in visualization: management of the
enormous amounts of data. We focus on the trade-off between two prominent dimensions of data
reduction, resolution and precision, with respect to common analysis and visualization tasks. To
keep the study tractable while not compromising the generalizability of results, we target
fundamental analysis and visualization tasks, with an outlook that these can serve as building
blocks for more complex and multiparameter tasks in the future. Although the paper focuses on a
small set of core tasks, the framework is generic and applies to any well-defined metric, and one
future direction is to consider a broader set of tasks.

We present the first empirical study to demonstrate that combining reduction in precision and
resolution can lead to a significant improvement in data quality for the same data budget, and that
different tasks might prefer different resolution-versus-precision trade-offs. For example, whereas
computing histograms requires high precision, computing derivatives benefits more from higher
resolution, and function reconstruction and isosurface extraction require a suitable mix of the two
(see~\Cref{fig:bit-distrib}). We also show that common reduction techniques, e.g., those based on
\slvl and \smag, do not perform well when leading zero bits are removed (to simulate entropy
compression). For each task, the relative ordering of the rate-distortion curves stays largely the
same regardless of data sets, although the gaps among them vary depending on the smoothness and
noisiness of the data. Compared to data-independent streams, signature-based streams often perform
better because they can adapt to the data. They are also amenable for implementation (unlike \sopt),
since a signature is negligibly small and thus can be precomputed and stored during preprocessing.
It is also interesting to consider per-block signatures instead of a global one.

An important question is whether task- and data-dependent streams provide sufficient advantages over
purely data-independent streams. In practice, data would be used for multiple, and not necessarily
predefined, tasks, and maintaining multiple streams will likely lead to additional overheads. Here,
we consider \ssig to be the best possible stream that could be realized. Improvements on \ssig in
the resolution-versus-precision space are likely possible, but they are unlikely to be significant.
Given these assumptions and the fact that \ssig in most cases provides very similar results to \sbit
or \swav, the additional effort (and potential overheads) for task-dependent bit orderings is
unlikely to be beneficial. This leaves a significant gap between the best data-independent streams
and the optimal stream \sopt. Our experiments suggest that the majority of these differences can be
attributed to spatial adaptivity (see~\Cref{fig:bit-distrib}). The prototypical example is
isosurface computation, where \sopt can skip all regions that do not affect any portion of the
surface. It may be worthwhile in future work to investigate solutions to spatial adaptivity to
significantly improve the performance of data-independent streams.

This study can be considered only a first step toward a system of solutions that can optimize
storage, network, and I/O bandwidth to suit specific tasks at hand. Ultimately, our results can
guide development of new data layouts and file formats for scientific data. \swav appears to provide
the best all around performance. However, for a file format additional constraints must be
considered, such as disk block sizes, cache coherence, and compression. Furthermore, an ideal format
should allow task-dependent data queries even though task-dependent formats will likely be
restricted to very specific situations. 

Finally, for fair comparisons, we always reconstruct the data at full resolution using wavelets.
However, processing and memory costs are important, and it is likely that adaptive representations
would be used in practice~\cite{gigavoxels,Gobbetti2008,vdb2013}. In these cases the error of a
given approximation depends not only on the available information, but also on the data structure
and algorithm being used. For example, trilinear interpolation on a coarse grid might produce
different results than wavelet reconstruction on the original mesh. There exist solutions where both
interpolations are equivalent~\cite{weiss}, but such solutions have not yet been implemented in
standard tools. An important future research direction will be to understand the implications of the
results presented here for existing toolchains such as VTK.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "template"
%%% End:
