\section{Discussion}

We present the first empirical study to demonstrate that: Combining precision and resolution can lead to significant advantages; and that different task prefer different resolution-versus-precision tradeoffs. 
For example, while histograms require high-precision computing derivatives benefits more from higher spatial resolution.
For brevity we focus on on a small set of core tasks with a single metric for each task. 
However, the framework is generic and applies to any well defined metric and one future direction is to consider a broader set of of domain specific tasks. 
One common task we did not address is volume rendering. 
While it is easy to define an error metric, i.e.\ pixel differences of the resulting images, the results will heavily depend on the viewpoint and transfer function.
Hand-selecting these would bias the results, yet integrating results across all possible viewpoints and transfer functions is not useful, as many such images would likely not be practically relevant. 
However, our experience has been that in practice, many volume rendered images simply show several layers of translucent isosurfaces. 
For these cases, our results on isosurfaces provide a good stand-in.


Another important question is whether task and data dependent streams provide sufficient advantages compared to task- and data-independent ones. 
In practice, data would be used for multiple, not-necessarily predefined tasks and maintaining multiple streams will invariably lead to additional overheads. 
Furthermore, data-dependent streams imply at best a global pre-processing step as the data is stored and encoding (and communicating) an arbitrary order will likely negate any storage savings. 
Here, we consider \sopt to be the ideal data- and task-dependent stream and \ssig to be the best possible data-independent stream.  
In both case some improvements are likely possible, i.e.\ a better optimization for \sopt and different ideas for creating \ssig, but one would not expect these to lead to fundamentally different results. 
Given these assumptions and the fact that \ssig in most cases provides very similar results to \sbit or \swav the additional effort (and potential overheads) for task-dependent orderings it unlikely to be beneficial.
This leaves a significant gap between the good static streams, in particular \swav and \sbit, and the optimal stream \sopt. 
Interestingly, additional experiments suggest that the majority of this difference can be attributed to spatial adaptivity. 
The prototypical example are iso-surfaces where \sopt can skip all regions that do not effect any portion of the surface.  
Yet, in regions that do contain relevant data \swav and \sbit often perform very similar to \sopt. 
Ultimately, our results can guide the development of new data layouts and file formats for scientific data. 
Generically, it seems \swav provides the best all around performance especially if it could be coupled with a spatially adaptive hierarchy. 
Note, however, that for a file format additional constrains must be considered, such as disk block sizes and cache coherence. 
Furthermore, an ideal format should allow task-dependent queries even though, as discussed above, task-dependent formats will likely be restricted to very specific situations. 

Another important consideration is that to provide a fair comparison we always reconstruct the data at full resolution using the appropriate basis functions. 
This isolates the question of how much information the data contains from how it is represented. 
However, in practice, processing and memory cost would also be important factors and given highly adaptive streams it is likely that adaptive representations would be used. 
The most natural examples are multi-resolution formats~\cite{gigavoxels,Gobbetti2008,vdb2013} which can represent sparse grids. 
In these cases the error of a given approximation depends not only on the information that is available, but also on what algorithm and what interpolation is used by the corresponding post-processing. 
For example, assuming trilinear interpolation on a coarse grid, especially an adaptive grid with T-junctions, might provide much different results than the corresponding wavelet interpolation on the original mesh. 
At the same time it is possible to specifically construct grids where both interpolations are equivalent~\cite{weiss} yet these are not yet implement in standard tools. 
An important future research direction will be to understand the implications of the results presented here on existing tool chains, i.e.\ VTK. 




% For a given task, the signature-based stream (\ssig) computed from an \sopt can be viewed as a
% benchmark for the data-independent streams. In practice, task-dependent signatures can also be
% precomputed and used to guide streaming. However, we show that in most cases, the performance of
% \ssig can be well approximated by either \emph{by bit plane} (\sbit), or \emph{by wavelet norm}
% (\swav), suggesting that it might be not fruitful to pursue an ``ideal'' bit ordering in the
% resolution-versus-precision space. Instead, for substantial gain, effort should be spent on bridging
% the gap between \sopt and the others, which mostly concerns spatial adaptivity. For example,
% isosurface extraction can benefit substantially from a data structure that can localize an isosurface
% in spatial domain, saving bits from being read and streamed elsewhere.

% Although we always reconstruct the data at full resolution for straightforward comparison, it is
% often infeasible for systems in practice to always work in full resolution. Most often, some form of
% adaptive-refinement-mesh is used, where density of data points varies spatially. Such configurations
% call for a principled method for measuring error between grids of different resolutions, which we
% believe will need further investigation.

% The results presented here can help designing file formats for scientific data. For example, the
% \emph{by wavelet norm} (\swav) stream might be a reasonable candidate for a default bit ordering,
% for its all-around good performance. However, for a file format, there are often conflicting goals
% and tradeoffs that need balancing. For example, further work is required to decide which set of bits
% should be stored together, and how best to encode (compress) them so that I/O cost is minimized.
% High compression ratios and efficient disk I/O are often in conflict with the fine-granularity of
% spatial adaptivity and random access, which concerns blocking and the size of blocks. Finally, a
% good file format should work well for combinations of tasks, which we do not address.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "template"
%%% End:
