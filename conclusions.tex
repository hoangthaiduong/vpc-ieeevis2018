\section{Discussion and Future Work}

{\color{blue}This work addresses one of the biggest challenges in scientific visualization today,
which is the enormous amount of data being generated. We focus on an issue rarely discussed in the
literature, which is the trade-off between two prominent dimensions of data reduction, namely
resolution and precision, when performing analysis tasks. To keep the study tractable while not
compromising the generalizability of results, we avoid multi-parameter tasks such as volume
rendering, in hopes that the fundamental analysis tasks considered here can serve as building blocks
for more complex tasks. Although we are more concerned with the quality of underlying data rather
than visualization outputs, we believe the two often strongly correlate, as demonstrated throughout
the paper. We have also chosen error metrics to measure data quality so that they can be good
proxies for evaluating visualization outputs. The paper focuses on a small set of core tasks, but
the framework is generic and applies to any well-defined metric, and one future direction is to
consider a broader set of tasks.}

We present the first empirical study to demonstrate that combining reduction in precision and
resolution can lead to a significant improvement in data quality for the same data size, and that
different tasks might prefer different resolution-versus-precision trade-offs. For example, while
computing histograms requires high precision, computing derivatives benefits more from higher
resolution, and function reconstruction and isosurface extraction require a proper mix of the two
(see~\Cref{fig:bit-distrib}). We also show that common reduction techniques, e.g., those based on
\slvl and \smag, do not perform well when leading zero bits are removed (to simulate entropy
compression). {\color{red}For each task, the relative ordering of the rate-distortion curves stay
largely the same regardless of data sets, although the gaps between them vary depending on the
smoothness and noisiness of the data. Compared to data-independent streams, signature-based streams
often perform better because they are more adaptive to the data. They are also amenable for
implementation (unlike \sopt), as a signature is negligibly small and thus can be precomputed and
stored during pre-processing. It is also interesting to consider per-block signatures instead of a
global one.}

{\color{red}An important question is whether task- and data-dependent streams provide sufficient
advantages over purely data-independent streams. In practice, data would be used for multiple,
not-necessarily predefined tasks and maintaining multiple streams will likely lead to additional
overheads. Here, we consider \ssig to be the best possible stream that could be realized.
Improvements on \ssig in the resolution-versus-precision space are likely possible, but unlikely to
be significant. Given these assumptions and the fact that \ssig in most cases provides very similar
results to \sbit or \swav, the additional effort (and potential overheads) for task-dependent bit
orderings is unlikely to be beneficial. This leaves a significant gap between the best
data-independent streams and the optimal stream \sopt. Our experiments suggest that the majority of
this difference can be attributed to spatial adaptivity. The prototypical example are isosurfaces
where \sopt can skip all regions that do not effect any portion of the surface. It is worthwhile in
future work to investigate solutions to spatial adaptivity to significantly improve the performance
of data-independent streams.}

This study can be considered only a first step towards a system of solutions that can optimize
storage, network, and I/O bandwidth to suit specific tasks at hand. Ultimately, our results can
guide development of new data layouts and file formats for scientific data. It seems \swav provides
the best all around performance. However, for a file format additional constraints must be
considered, such as disk block sizes, cache coherence, and compression. Furthermore, an ideal format
should allow task-dependent data queries even though task-dependent formats will likely be
restricted to very specific situations. 

Another important consideration is that for fair comparisons we always reconstruct the data at full
resolution using wavelets. However, in practice, processing and memory costs are important, and it
is likely that adaptive representations would be used~\cite{gigavoxels,Gobbetti2008,vdb2013}. In
these cases the error of a given approximation depends not only on the information that is
available, but also on what algorithm and what interpolation method are used. For example,
trilinear interpolation on a coarse grid might provide vastly different results than wavelet
reconstruction on the original mesh. It is possible to specifically construct grids where both
interpolations are equivalent~\cite{weiss}, yet these are not yet implement in standard tools. An
important future research direction will be to understand the implications of the results presented
here on existing toolchains such as VTK.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "template"
%%% End:
