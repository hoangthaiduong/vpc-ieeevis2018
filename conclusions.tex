\section{Discussions}

Our study shows that different tasks potentially require different resolution-versus-precision
tradeoffs. In particular, while computations of histogram requires high-precision data, computations
of derivatives require high-resolution data. Good reconstructions of data in $L_2$ norm require a
proper mix of resolution and precision (see~\Cref{fig:bit-distrib}). One notable omission is volume
rendering, which poses a serious challenge as it depends on many factors such as the transfer
function used and the camera positions. However, our experience has been that in practice, many
volume rendered images simply show several layers of translucent isosurfaces. In such cases, our
results for isosurface extraction might apply.

Although we focus on a small set of core tasks in this paper, and only pick one metric for each
task, the framework we propose is generic enough to work for any well defined error metric.It
remains future work to apply the same methodologies to more advanced, or domain-specific tasks.

For a given task, the signature-based stream (\ssig) computed from an \sopt can be viewed as a
benchmark for the data-independent streams. In practice, task-dependent signatures can also be
precomputed and used to guide streaming. However, we show that in most cases, the performance of
\ssig can be well approximated by either \emph{by bit plane} (\sbit), or \emph{by wavelet norm}
(\swav), suggesting that it might be not fruitful to pursue an ``ideal'' bit ordering in the
resolution-versus-precision space. Instead, for substantial gain, effort should be spent on bridging
the gap between \sopt and the others, which mostly concerns spatial adaptivity. For example,
isosurface extraction can benefit subtantially from a data structure that can localize an isosurface
in spatial domain, saving bits from being read and streamed elsewhere.

Although we always reconstruct the data at full resolution for straightforward comparison, it is
often infeasible for systems in practice to always work in full resolution. Most often, some form of
adaptive-refinement-mesh is used, where density of data points varies spatially. Such configurations
call for a principled method for measuring error between grids of different resolutions, which we
believe will need further investigation.

The results presented here can help designing file formats for scientic data. For example, the
\emph{by wavelet norm} (\swav) stream might be a reasonable candidate for a default bit ordering,
for its all-around good performance. However, for a file format, there are often conflicting goals
and tradeoffs that need balancing. For example, further work is required to decide which set of bits
should be stored together, and how best to encode (compress) them so that I/O cost is minimized.
High compression ratios and efficient disk I/O are often in conflict with the fine-granularity of
spatial adaptivity and random access, which concerns blocking and the size of blocks. Finally, a
good file format should work well for combinations of tasks, which we do not address.


