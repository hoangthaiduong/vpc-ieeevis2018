\section{Related work}

Techniques that reduce data in resolution typically build a tree-shape hierarchy over the data. A
very common scheme to genearte such a hierarchy is to construct low-resolution copies of the data
from higher-resolution ones through filtering and subsampling. Examples include Gaussian and
Laplacian pyramids~\cite{laplacian-pyramid},
mipmaps~\cite{multires_octree1999,interactive-exploration-ct-scans}. Often times, the data is stored
in blocks on each resolution level. To save bandwidth, low-resolution blocks can be streamed during
rendering, if the points being queried project to less than a pixel on screen. However, these methods
increase storage requirements, making them unsuitable for very large data.

Recent multi-resolution techniques save storage by adapting to the data in such a way that different
regions of the data are stored in different resolutions, depending on how ``homogeneous'' the region
is. Fast-varying regions are stored at higher resolution. A very popular approach is sparse voxel
octrees (SVO), pioneered by Crassin \etal~\cite{gigavoxels} and Gobbetti \etal~\cite{Gobbetti2008},
and variations of which are found in~\cite{Fogal-2013-RayGuided,visualization-driven}. Sparsity
comes from the fact that smooth-varying regions are stored at coarser octree levels, which
significantly reduces storage. During rendering, blocks of samples are streamed from an appropriate
resolution, determined by how far the queried samples are from the eye/camera. Beyer
\etal~\cite{large‚Äêscale-volume} gives a comprehensive overview of state-of-the-art GPU-based
out-of-core ray-casting techniques.

Beside octrees, other trees such as B+ tree~\cite{vdb2013} and
kd-tree~\cite{fogal-kdtree,in-situ-sampling-particle} can also be used to build a sparse hierarchy.
Alternatively, space-filling curves such as the hierarhical Z curve~\cite{idx2001} or the Hillbert
curve~\cite{mloc} can be used to reorder data samples to form a hierarchy without any filtering
steps or redundant samples, as low-resolution levels are constructed via subsampling. Unfortunately,
subsampling can introduce heavy aliasing artifacts.

Other multiresolution approaches reduce data by transform-based compression. For example,
COVRA~\cite{covra2012} constructs an octrees of bricks, which are further subdivided blocks.
Compression is done by learning a sparse representation for the blocks in terms of prototype (basis)
blocks. Similarly, Fout\etal~\cite{hw_dvr2007} transform each block in the volume using the KLT
transform, which produces several classes of partitions (each can be thought of as one resolution
level). They compress the transform coefficients using vector quantization~\cite{vq1992}, by
constructing one codebook for each paritition class. Schneider\etal~\cite{compression_domain2003}
also uses vector quantization on transform coefficients, but with a simple Haar-like transform that
separates each block of $2^3$ voxels into one average and seven differences.

Analogous to the KLT transform in 2D, the Tucker decomposition~\cite{tensor_dvr2015} in
$n$-dimensional space decomposes the input data (stored as a tensor) into $n$ matrices of basis
vectors and one core tensor. Reduction in storage and in transmission bandwidth comes from the fact
that the basis matrices can be downsampled (resulting in a lower-resolution representation), the
core tensor can be rank-truncated (resulting in coarse-scale representations of
features)~\cite{tamresh,tucker-thresholding,multiscale-tensor}. Furthermore, elements in the basis
matrices and the core tensor can be thresholded~\cite{tucker-thresholding} or
quantized~\cite{tamresh,multiscale-tensor}. Tensor decomposition can work for higher-dimensional
data, and can achieve very high ratios of quality over data size. However, the transform step is
highly data-dependent, thus relatively costly.

Transforms that use fixed bases avoid such high computation cost, at the expense of slightly less
effective compression. Perhaps the most popular transform that uses a fixed basis is the (discrete)
wavelet transform (DWT). The DWT constructs a hierarchy of resolution levels via low and high
bandpass filters. The transform is recursively applied to the lower-resolution band, resulting in a
hierarchy of ``details'' at varying resolution. One benefit of wavelets over redundant
representations such as Laplacian pyramids is that the wavelet transform is merely a change of basis
that like reordering techniques does not increase the data size. Another benefit over adaptive
refinement meshes and other tree-like techniques is that the wavelet basis functions are defined
everywhere in space, requiring no special interpolation rules when given some arbitrary subset of
wavelet coefficients and basis functions. One disadvantage of the wavelet transform is non-constant
time random access cost, though acceleration structures have been proposed to speed up local
queries~\cite{weiss}.

Beside offering a multiresolution decomposition, enabling data streaming with level-of-detail
support, wavelets also offer ample opportunities for compression. As the magnitude of wavelet
coefficients decay rapidly for a typical volume [CITE], they are especially amenable to thresholding
or entropy compression. In the context of storing and visualizing scientific data, wavelets (with
compression) are used in a wide variety of systems
(~\cite{multires_toolkit2003,vapor2007,woodring2011}), and applications (volume rendering with
level-of-detail~\cite{wavelet-compression-interactive-vis,multires-framework,rapid-compression-volume,interactive-rendering-large-volume,multires-volume-rendering},
turbulence visualization~\cite{treib}, particle visualization~\cite{sph-octree}).

Most wavelet-based techniques employ tiling of wavelet coefficients in individual subands to
facilitate random access, and spatial adaptivity in resolution. For example, the VAPOR
toolkit~\cite{vapor2007} incorporates a multiresolution file format based on a wavelets to allow
data analysis on commodity hardware, and stores individual tiles in separate files to allow loading
of the region of interest. However, like most multiresolution work, only the resolution control is
leveraged. The precision axis, which can potentially further reduce data transfer, is left
unexplored.

Most work that explores the precision axis comes from state-of-the-art coders for wavelet
coefficients in image compression. Wavelet coefficients in corresponding regions across subbands can
be thought of as belonging to a ``tree'', with the root being a single coefficient at the lowest
subband. The Embedded Zero Trees (EZW) coder exploits the property that in such trees, ``parent''
coefficients are often larger in magnitude than ``child'' coefficients. It locates trees of wavelet
coefficients that are insignificant with regard to (i.e., less than in magnitude) a threshold. Such
a tree is encoded with one single symbol, resulting in significant compression. The threshold is
typically set at each bit plane, starting from the most significant one. In this way, the data can
be progressively refined in precision during decompression. The SPIHT coder~\cite{spiht1996}
improves on EZW by locating more general types of zero trees~\cite{quantifying-coding-performance}.
SPECK~\cite{speck2004} extends SPIHT to exploit also spatial correlations among nearby coefficients
on the same subband.

\newcommand{\zfp}{\textsc{zfp}\xspace}
The \zfp compression scheme~\cite{zfp2014} also codes transform coefficients by bit plane, in order
of decreasing significance. \zfp partitions the domain---a structured grid---into small (e.g., $4
\times 4 \times 4$) independent blocks and thus allow for localized decompression. Moreover, \zfp
supports fixed-rate compression, which facilitates random access to the data. Its fast transform and
caching of decompressed data allows it to achieve not only high throughput decompression, but also
fast inline compression. Extensions of \zfp allow it to vary either the bit rate or precision
spatially over the domain, albeit at fixed resolution~\cite{zfp-arc}. Other notable compression
schemes for scientific data include ISABELA~\cite{isabela}, SZ~\cite{sz}, and FPZIP~\cite{fpzip}.
All three techniques employ prediction and compress the residuals. ISABELA and SZ perform residual
scalar quantization, while FPZIP truncates floats, which can be seen as nonuniform scalar
quantization. Another work that uses scalar quantization is~\cite{sqe}. Similar to FPZIP, the
Precision-based Level of Details (PLoD) scheme proposed in MLOC~\cite{mloc} also truncates floats by
dividing a double-precision number into seven parts, of which the first part contains the first two
bytes, and each of the other six parts contains one byte for additional precision. MLOC also
proposes a multiresolution scheme based on Hillbert curves, but this scheme (based on resolution)
and the PLoD scheme (based on precision) are exclusive.

Schemes that allow progressive data access in both resolution and precision include
SBHP~\cite{sbhp2000} and JPEG2000~\cite{jpeg2000}. Both partition each subband into blocks and code
each block independently, in bit plane order. By interleaving compressed bits across blocks, one can
construct a purely resolution-progressive or a purely precision-progressive stream, or anything in
between. Outside out image compression, JPEG2000 has found use in compression of scientific data.
For example, Woodring \etal~\cite{woodring2011} use JPEG2000 to store floating point data. Since
most JPEG2000 implementations are limited to integer data, the authors apply uniform scalar
quantization to convert floating point data to integer form. Even though JPEG2000 supports varying
both resolution and precision, the authors do not explore this capability but focus only on setting
a bit rate. In general, there has not been work that studies the resolution-versus-precision
tradeoffs, as well as the various orderings of bits in the context of data analysis and
visualization.

by magnitude streams~\cite{image_compression1992}

Transfer function adaptive decompression~\cite{tf_decompression2004}
\peter{We might want to cite~\cite{codar} for surveys on data reduction.}

Error analysis:
~\cite{evaluating-compression-climate}
~\cite{compression_sim2013}
~\cite{statistical-volume-quality}
~\cite{evaluating-efficacy-wavelet}
~\cite{topology-verification-isosurface}
~\cite{verifiable-isosurface}
~\cite{verifying-volume-rendering}
~\cite{statistical-volume-quality}

error-guided: 
~\cite{tf_decompression2004} (based on transfer function)

surveys:
~\cite{state-of-the-art-compressed-volume} (compressed volume rendering)
~\cite{li2018} data reduction techniques
