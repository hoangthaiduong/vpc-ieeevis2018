\section{Related work}

\para{Tree-based multiresolution hierarchies.}
A very common scheme to generate a tree-like hierarchy is to construct low-resolution copies of the
data from higher resolution ones through downsampling. Examples include Gaussian and Laplacian
pyramids~\cite{laplacian-pyramid},
mipmaps~\cite{multires_octree1999,interactive-exploration-ct-scans}. Often, the data is stored in
blocks on each resolution level. To save bandwidth, low-resolution versions of distant blocks can be
streamed during rendering. However, these methods increase storage requirements, making them
unsuitable for very large data.

Recent multi-resolution techniques save storage by adapting to the data in such a way that different
regions are stored with different resolutions. A very popular approach is sparse voxel
octrees~\cite{gigavoxels,Gobbetti2008}, and
variations~\cite{Fogal-2013-RayGuided,visualization-driven,large‚Äêscale-volume}. Smooth-varying
regions are stored at coarser octree levels, which significantly reduces storage. During rendering,
blocks of samples are streamed from an appropriate resolution, determined by how far the queried
samples are from the eye/camera.

A sparse, multiresolution hierarchy can also be built using other trees such as B+
tree~\cite{vdb2013} and kd-tree~\cite{fogal-kdtree,in-situ-sampling-particle}, or space-filling
curves~\cite{idx2001, mloc}, which reorder data samples to form a hierarchy without any filtering
steps or redundant samples. Low-resolution levels are constructed via subsampling, which,
unfortunately, is prone to aliasing problems.

\para{Transform-based multiresolution hierarchies.}
Other multiresolution approaches reduce data by transform-based compression. For example,
COVRA~\cite{covra2012} constructs an octree of bricks (consisting of blocks), and learn a sparse
representation for the blocks in terms of basis blocks. Similarly, Fout \etal~\cite{hw_dvr2007}
transform each block using the KLT transform, producing several resolution levels, and compress each
level using vector quantization~\cite{vq1992}. Schneider~\etal~\cite{compression_domain2003} also
use vector quantization, but with a simple Haar-like transform that separates each block of $2^3$
voxels into one average and seven difference coefficients. Other examples of transform-based
hierarchies include works that use the Tucker
decomposition~\cite{tensor_dvr2015,tamresh,tucker-thresholding,multiscale-tensor}, which decomposes
the input data (stored as a tensor) into $n$ matrices of basis vectors and one core tensor. Tensor
decomposition works for higher dimensional data, and can achieve high compression ratios, albeit at
the price of a costly transform step.

%\paragraph{\textbf{Wavelets}}
\para{Wavelets.}
Transforms that use fixed bases avoid such high computation cost at the expense of slightly less
effective compression. Perhaps the most popular transform that uses a fixed basis is the (discrete)
wavelet transform (DWT), which constructs a hierarchy of resolution levels via low and high bandpass
filters. The transform is recursively applied to the lower resolution band, resulting in a hierarchy
of ``details'' at varying resolution. The DWT is merely a change of basis that does not increase the
data size. Furthermore, the wavelet basis functions are defined everywhere in space, requiring no
special interpolation rules when given some arbitrary subset of wavelet coefficients. One
disadvantage of the DWT is the random access cost, which is not constant time, although there has
been work to develop acceleration structures to speed up local queries~\cite{weiss}.

Beside offering a multiresolution decomposition, enabling data streaming with level-of-detail
support, wavelet coefficients are especially amenable for compression, through thresholding or
entropy compression. In storing and visualizing scientific data, wavelets (with compression) are
used in a wide variety of systems~\cite{multires_toolkit2003,vapor2007,woodring2011} and
applications such as volume rendering with
level-of-detail~\cite{wavelet-compression-interactive-vis,multires-framework,rapid-compression-volume,interactive-rendering-large-volume,multires-volume-rendering},
turbulence visualization~\cite{treib}, and particle visualization~\cite{sph-octree}.

Most wavelet-based techniques employ tiling of wavelet coefficients in individual subands to
facilitate random access and spatial adaptivity in resolution. For example, the VAPOR
toolkit~\cite{vapor2007} incorporates a multiresolution file format based on wavelets to allow
data analysis on commodity hardware by storing individual tiles in separate files to allow loading
of the region of interest. However, like most multiresolution work, only the resolution control is
leveraged. The precision axis, which can potentially further reduce data transfer, is left
unexplored.

%\paragraph{\textbf{Wavelet coders}}
\para{Wavelet coders.}
Most work that explores the precision axis comes from state-of-the-art wavelet coders. Wavelet
coefficients in corresponding regions across subbands can be thought of as belonging to a ``tree''.
The emmbedded zerotrees (EZW) coder exploits the property that in such trees, ``parent''
coefficients are often larger in magnitude than their ``children''. It locates trees of wavelet
coefficients that are insignificant with regard to a series of thresholds, and encode such a tree
with one single symbol. The thresholds are typically set at the bit planes, starting from the most
significant one. In this way, the data can be progressively refined in precision during
decompression. The SPIHT coder~\cite{spiht1996} improves on EZW by locating more general types of
zero trees~\cite{quantifying-coding-performance}. SPECK~\cite{speck2004} extends SPIHT to exploit
also spatial correlations among nearby coefficients on the same subband.

%\paragraph{\textbf{Floating-point compression}}
\para{Floating-point compression.}
\newcommand{\zfp}{\textsc{zfp}\xspace}
The \zfp compression scheme~\cite{zfp2014} also encodes transform coefficients by bit plane, in
order of decreasing significance. By partitioning the domain into $4 \times 4 \times 4$ independent
blocks, \zfp supports fixed-rate compression, random access to the data, localized decompression,
and fast inline compression. Extensions of \zfp allow it to vary either the bit rate or precision
spatially over the domain, albeit at fixed resolution~\cite{zfp-arc}. Other notable compression
schemes for scientific data include scalar quantization encoding (SQE)~\cite{sqe},
ISABELA~\cite{isabela}, SZ~\cite{sz}, and FPZIP~\cite{fpzip}, which generally employ prediction and
compress the residuals. ISABELA and SZ perform residual scalar quantization, whereas FPZIP truncates
floats, which can be seen as nonuniform scalar quantization. Similarly, the precision-based level of
details (PLoD) scheme proposed in MLOC~\cite{mloc} also truncates floats by dividing a
double-precision number into several parts. MLOC includes a multiresolution scheme based on Hillbert
curves, but this scheme (based on resolution) and the PLoD scheme (based on precision) are
exclusive.

%\paragraph{\textbf{Mixing resolution and precision}}
\para{Mixing resolution and precision.}
Schemes that allow progressive data access in both resolution and precision include
SBHP~\cite{sbhp2000} and JPEG2000~\cite{jpeg2000}. Both partition each subband into blocks and code
each block independently, in bit plane order. By interleaving compressed bits across blocks, one can
construct a purely resolution-progressive or a purely precision-progressive stream, or anything in
between. Outside image compression, JPEG2000 has found use in the compression of scientific data.
For example, Woodring \etal~\cite{woodring2011} use JPEG2000 to store floating point data. Since
most JPEG2000 implementations are limited to integer data, the authors apply uniform scalar
quantization to convert floating point data to integer form. Even though JPEG2000 supports varying
both resolution and precision, most works do not explore this capability but focus only on setting
bit rate. In general, efficiently leveraging both axes of data reduction has not been well-studied.

%\paragraph{\textbf{Error quantification}}
\para{Error quantification.}
Several works have aimed at quantifying the error incurred by data reduction, by compression or
otherwise, for the results of analysis tasks. Baker \etal~\cite{evaluating-compression-climate}
evaluate several compressors (FPZIP~\cite{fpzip}, APAX~\cite{apax}, ISABELA~\cite{isabela}) on
ensembles of climate simulation data. Laney \etal~\cite{compression_sim2013} study the effects of
lossy compression (using FPZIP and APEX) on the Miranda hydrodynamic simulation code. Li
\etal~\cite{evaluating-efficacy-wavelet} measure the error incurred by wavelet compression on
turbulent-flow data. Wang \etal~\cite{statistical-volume-quality} assess the quality of distorted
data using a combination of statistical metrics in the wavelet transform domain. Etiene \etal have
published a series of studies on verification of isosurfaces in
geometrical~\cite{verifiable-isosurface} and topological~\cite{topology-verification-isosurface}
terms, as well as verification of volume-rendered images~\cite{verifying-volume-rendering}. They
focus on order-of-accuracy and convergence analysis, where errors are introduced mostly through
discretization, not necessarily for the purpose of reducing data bandwidth. Finally, the Z-checker
framework~\cite{z-checker} consists of a wide range of independent metrics to evaluate data quality
after lossy compression. In general, the resolution-versus-precision tradeoffs, as well as the
various orderings of bits in data analysis and visualization have not been well studied.

%by magnitude streams~\cite{image_compression1992}
% Transfer function adaptive decompression~\cite{tf_decompression2004}

Finally, for surveys of data reduction techniques in general, we refer the readers to the work of
Rodr\'{\i}guez \etal~\cite{state-of-the-art-compressed-volume} and Li \etal~\cite{li2018}.
