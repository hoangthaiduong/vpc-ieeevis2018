\section{Related work}

Techniques that reduce data in resolution typically build a tree-shape hierarchy over the data. A
very common scheme to genearte such a hierarchy is to construct low-resolution copies of the data
from higher-resolution ones through filtering and subsampling. Examples include Gaussian and
Laplacian pyramids~\cite{laplacian-pyramid},
mipmaps~\cite{multires_octree1999,interactive-exploration-ct-scans}. Often times, the data is stored
in blocks on each resolution level. To save bandwidth, low-resolution blocks can be streamed during
rendering, if the points being queried project to less than a pixel on screen. However, these methods
increase storage requirements, making them unsuitable for very large data.

Recent multi-resolution techniques save storage by adapting to the data in such a way that different
regions of the data are stored in different resolutions, depending on how ``homogeneous'' the region
is. Fast-varying regions are stored at higher resolution. A very popular approach is sparse voxel
octrees (SVO), pioneered by Crassin \etal~\cite{gigavoxels} and Gobbetti \etal~\cite{Gobbetti2008},
and variations of which are found in~\cite{Fogal-2013-RayGuided,visualization-driven}. Sparsity
comes from the fact that smooth-varying regions are stored at coarser octree levels, which
significantly reduces storage. During rendering, blocks of samples are streamed from an appropriate
resolution, determined by how far the queried samples are from the eye/camera. Beyer
\etal~\cite{large‚Äêscale-volume} gives a comprehensive overview of state-of-the-art GPU-based
out-of-core ray-casting techniques.

Beside octrees, other trees such as B+ tree~\cite{vdb2013} and
kd-tree~\cite{fogal-kdtree,in-situ-sampling-particle} can also be used to build a sparse hierarchy.
Alternatively, space-filling curves such as the hierarhical Z curve~\cite{idx2001} can be used to
reorder data samples to form a hierarchy without any filtering steps or redundant samples, as
low-resolution levels are constructed via subsampling. Unfortunately, subsampling can introduce
heavy aliasing artifacts.

Other multiresolution approaches reduce data by transform-based compression. For example,
COVRA~\cite{covra2012} constructs an octrees of bricks, which are further subdivided blocks.
Compression is done by learning a sparse representation for the blocks in terms of prototype (basis)
blocks. Similarly, Fout\etal~\cite{hw_dvr2007} transform each block in the volume using the KLT
transform, which produces several classes of partitions (each can be thought of as one resolution
level). They compress the transform coefficients using vector quantization~\cite{vq1992}, by
constructing one codebook for each paritition class. Schneider\etal~\cite{compression_domain2003}
also uses vector quantization on transform coefficients, but with a simple Haar-like transform that
separates each block of $2^3$ voxels into one average and seven differences.

Analogous to the KLT transform in 2D, the Tucker decomposition~\cite{tensor_dvr2015} in
$n$-dimensional space decomposes the input data (stored as a tensor) into $n$ matrices of basis
vectors and one core tensor. Reduction in storage and in transmission bandwidth comes from the fact
that the basis matrices can be downsampled (resulting in a lower-resolution representation), the
core tensor can be rank-truncated (resulting in coarse-scale representations of
features)~\cite{tamresh,tucker-thresholding,multiscale-tensor}. Furthermore, elements in the basis
matrices and the core tensor can be thresholded~\cite{tucker-thresholding} or
quantized~\cite{tamresh,multiscale-tensor}. Tensor decomposition can work for higher-dimensional
data, and can achieve very high ratios of quality over data size. However, the transform step is
highly data-dependent, thus relatively costly.

Transforms that use fixed bases avoid such high computation cost, at the expense of slightly less
effective compression. Perhaps the most popular transform that uses a fixed basis is the (discrete)
wavelet transform (DWT). The DWT constructs a hierarchy of resolution levels via low and high
bandpass filters. The transform is recursively applied to the lower-resolution band, resulting in a
hierarchy of ``details'' at varying resolution. One benefit of wavelets over redundant
representations such as Laplacian pyramids is that the wavelet transform is merely a change of basis
that like reordering techniques does not increase the data size. Another benefit over adaptive
refinement meshes and other tree-like techniques is that the wavelet basis functions are defined
everywhere in space, requiring no special interpolation rules when given some arbitrary subset of
wavelet coefficients and basis functions. One disadvantage of the wavelet transform is non-constant
time random access cost, though acceleration structures have been proposed to speed up local
queries~\cite{weiss}.

Beside offering a multiresolution decomposition, enabling data streaming with level-of-detail
support, wavelets also offer ample opportunities for compression. As the magnitude of wavelet
coefficients decay rapidly for a typical volume [CITE], they are especially amenable to thresholding
or entropy compression. In the context of storing and visualizing scientific data, wavelets (with
compression) are used in a wide variety of systems
(~\cite{multires_toolkit2003,vapor2007,woodring2011}), and applications (volume rendering with
level-of-detail~\cite{wavelet-compression-interactive-vis,multires-framework,rapid-compression-volume,interactive-rendering-large-volume,multires-volume-rendering},
turbulence visualization~\cite{treib}, particle visualization~\cite{sph-octree}). Note that these
techniques often do not perform error analysis on progressive data streaming. Mention the use of
blocking for random access.

multires-volume-rendering: error estimation to increase resolution on the fly

Transfer function adaptive decompression~\cite{tf_decompression2004}
Transform Coding for HW-accel DVR~\cite{hw_dvr2007}
\peter{We might want to cite~\cite{codar} and~\cite{li} for surveys on data reduction.}

Error analysis:
~\cite{evaluating-compression-climate}
~\cite{compression_sim2013}
~\cite{statistical-volume-quality}
~\cite{evaluating-efficacy-wavelet}
~\cite{topology-verification-isosurface}
~\cite{verifiable-isosurface}
~\cite{verifying-volume-rendering}
~\cite{statistical-volume-quality}

error-guided: 
~\cite{tf_decompression2004} (based on transfer function)

surveys:
~\cite{state-of-the-art-compressed-volume} (compressed volume rendering)
~\cite{li2018} data reduction techniques

compression:
~\cite{isabela}
~\cite{fpzip}
~\cite{sz}
~\cite{zfp2014}

precision:
~\cite{ezw}
~\cite{spiht1996}
~\cite{mloc}
~\cite{sbhp2000}
~\cite{jpeg2001}

Spatial adaptivity in resolution can be achieved by tiling the wavelet coefficients of individual
subbands. For example, the Visualization and Analysis Platform for Ocean, Atmosphere, and Solar
Researchers (VAPOR) toolkit~\cite{multires_toolkit2003, vapor2007} incorporates a multiresolution
file format based on a wavelets to allow data analysis on commodity hardware, and stores individual
tiles in separate files to allow loading of the region of interest. However, the authors only
leverage the resolution control without exploring the precision axis, which can potentially further
reduce data transfer.

The SPIHT~\cite{spiht1996} wavelet coefficient coding algorithm hierarchically partitions sets of
spatially related wavelet coefficients, exploiting the property that ``parent'' coefficients are
often larger in magnitude than ``child'' coefficients. This format allows regions to be
progressively refined in precision by coding more significant bitplanes before less significant
ones.

SBHP~\cite{sbhp2000}
JPEG2000~\cite{jpeg2001}
by bit plane streams~\cite{compression_techniques1991} (this is what SPIHT paper cites).
by magnitude streams~\cite{image_compression1992}
\peter{I'm not familiar with these last two papers.  Instead, point out that
SPIHT improves on embedded zerotree coding~\cite{ezw}.}

\peter{Discuss scalar~\cite{sqe} vs. vector quantization~\cite{hvq}. SZ performs residual scalar
quantization~\cite{sz}. \cite{fpzip} truncates floats, which can be seen as nonuniform scalar
quantization.}

\newcommand{\zfp}{\textsc{zfp}\xspace}
Block transform-based techniques such as \zfp~\cite{zfp2014} partition the domain---a structured
grid---into small (e.g., $4 \times 4 \times 4$) independent blocks and thus allow for localized
decompression. Moreover, \zfp supports fixed-rate compression, which facilitates random access to
the data. Its fast transform and caching of decompressed data allows it to achieve not only high
throughput decompression~\cite{hvq}, but also fast inline compression. Extensions of \zfp allow it
to vary either the bit rate or precision spatially over the domain, albeit at fixed resolution.
\peter{Not sure if we want to cite my ARC poster on this: https://computation.llnl.gov/sites/default/files/public//llnl-post-728998.pdf}

\peter{Should the following go in a section on ``hybrid'' precision and resolution techniques?}

Woodring \etal~\cite{woodring2011} use the JPEG2000 image format to store floating point data.
Since most JPEG2000 implementations are limited to integer data, the authors apply uniform scalar
quantization to convert floating point data to integer form. Even though JPEG2000 supports varying
both resolution and precision, the authors do not explore this capability but focus only on setting
a bit rate.
