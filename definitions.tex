\section{Data Reduction Schemes as Packet Streams}\label{sec:terminologies}

In order to systematically study the resolution-versus-precision trade-offs among different data
reduction schemes, it is important to perform fair and consistent comparisons. In this section, we
develop such a consistent methodology by proposing to model different data reduction schemes as
\emph{streams} of uniformly sized data \emph{packets}, where the original data contains all the
packets, and any reduction step removes a set of packets (comparable amounts of data). These data
streams are transmitted using a client-server model. At any point, the client is assumed to have
received a subset of packets (in some predetermined sequential order or in some order requested by
the client), which can be used to reconstruct an approximation to the original data. Therefore, to
compare different streams, we reconstruct the original data using the \emph{same number} of packets
from each stream and perform desired tasks on each of the (approximate) reconstructions. A stream is
considered better suited for a given task if it produces results that are closer to the reference
results computed from the original data. \Cref{fig:pipeline} gives a schematic view of our data
streaming model.

\begin{figure}[!b]
\vspace{-1em}
\centering
\includegraphics[width=\linewidth]{img/pipeline.png}
\vspace{-1em}
\caption{Our data streaming model. The input is a regular grid of floating-point samples;
the output is a stream of \emph{packets}. A packet consists of bits from the same bit plane, from a
block of negabinary wavelet coefficients. Different data reduction schemes generate different
streams.  The wavelet subbands are separated by blue lines in the second image, with the coarsest
subband at the top left corner. 
%Although not shown here, 
Quantization and negabinary conversion
happen immediately after wavelet transform.
%\todo{PL:Please do.  I donâ€™t know what the rightmost two subfigures show.  I also cannot read 1-point font.}
}\label{fig:pipeline}
\end{figure}

Although both the server and the client in our model can be on the same physical machine, only the
server has full knowledge of the data. Thus, when the client receives a packet, it might not know
where that packet should be deposited. A common solution is to have both the client and the server
agree beforehand on a static ordering of packets, independent of the data. We use the term
\emph{data-independent streams} to refer to streams using such solutions. In contrast, for
\emph{data-dependent streams}, an additional mechanism is needed to inform the client about the
subbands and bit planes of incoming packets. In this paper, we consider both types of streams, as
well as specialized \emph{task-dependent} streams optimized for given tasks (see
\Cref{tbl:streams}).

\begin{table}[!t]
\setlength\tabcolsep{4.5pt} % default value: 6pt
\centering
\begin{tabular}{l l c c}
\toprule
Symbol & Name & Data Dependent & Task Dependent \\
\midrule
\slvl & \emph{by level} & \xmark & \xmark\\
\sbit & \emph{by bit plane} & \xmark & \xmark\\
\swav & \emph{by wavelet norm} & \xmark & \xmark\\
\smag & \emph{by magnitude} & \cmark & \xmark\\
\stkop & \emph{task-optimized} & \cmark & \cmark\\
\stksg & \emph{by signature} & \cmark/\xmark & \cmark/\xmark\\
\bottomrule
\end{tabular}
\vspace{-0.5em}
\caption{We define various types of data streams, including data-independent, data-dependent,
task-independent, and task-dependent streams. \stksg can be data dependent or task dependent,
depending on the stream from which it is derived.\label{tbl:streams}}
\vspace{-2em}
\end{table}

\subsection{Decomposition of Data into Packets} \label{sec:data-streaming-framework}

Although one way to compare different data reduction strategies is to restrict the techniques to the
same data size and compare data quality, it is difficult to enforce consistency. For example, the
amount of change (in data) in one step of multiresolution simplification may be different from
removing one bit in the quantization of every sample. To make all data reduction schemes comparable,
in each scheme, a data set is redefined as a stream of equally sized \emph{packets}. These packets
are the smallest units of data transfer in our framework. A packet consists of a relatively small
number $\left(\approx 2^3\right)$ of bits and is associated with a resolution level and a precision
level (i.e., bit plane). In this framework, different data-reduction schemes become different
orderings of packets, called \emph{streams}. Restricting two (or more) data streams to the same
number of packets allows us to perform fair and consistent comparisons.

%\para{Packets for Different Resolution Levels.} 
\para{Resolution levels.} 
Although there exist several ways to define the notion of resolution/scale/frequency, we choose the
multilevel basis functions of the wavelet transform because they have compact support, and they
avoid interpolation problems associated with other representations. Wavelet transform enables
spatial adaptivity (i.e., finer resolution in regions that contain sharp features, at the expense of
coarser resolution elsewhere). In particular, we choose the CDF5/3 multilinear
wavelets~\cite{cdf-wavelets} for their balance between simplicity and effectiveness at decorrelating
the input signal in practice~\cite{jpeg2000}.

A multidimensional wavelet transform can be performed in multiple passes, which partitions the
original domain into \emph{subbands}, each of which can be thought of as a resolution level
associated with one or more spatial direction. One transform pass (in 3D) creates eight subbands, of
which the first is a low-pass, downsampled version of the original data, and the remaining add fine
details in each subset of the dimensions (see \Cref{fig:pipeline} for a visualization of subbands in
2D). A subsequent transform pass recurses only on the first subband (of the previous level),
creating the next (resolution) level of subbands. We use $l$ $(0 \leq l < L)$ to index the subbands,
with $l = 0$ referring to the coarsest subband and $L$ denoting the number of subbands. In 3D, the
eight subbands created after one transform pass are indexed in the following order, from coarse to
fine: LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH (L stands for ``low'' and H stands for ``high'',
referring to the low- and high-pass filter pair that perform the wavelet transform). The LHL
subband, for example, contains coefficients that are low-pass transformed along $X$ and $Z$, and
high-pass transformed along $Y$. In our experiments, the number of subbands, $L$, is fixed at $1 +
3*7=22$, corresponding to three transform passes in 3D.

%\para{Packets for Different Precision Levels.} 
\para{Precision levels.} 
For creating packets corresponding to different precision levels, we quantize floating-point wavelet
coefficients to $B$-bit signed integers. For most of the experiments in this paper, $B=16$. This
quantization eliminates the floating-point exponent bits, such that every bit (except the sign bit)
can be associated with a bit plane $b$ ($0\leq b < B$). In our convention, the higher indexed bit
planes are less significant. We convert quantized coefficients to the negabinary representation,
where integers are represented in base $-2$, i.e., $\sum_{b=0}^{B}{c_b(-2)^b}$ with $c_b\in
\{0,1\}$. Negabinary encoding is preferred over two's complement encoding, because we start data
reconstruction by zero-initializing all bits, and negabinary encoding has no single dedicated sign
bit and ensures that small coefficients have many leading zero-bits.
%
This transformation increases the number of bit planes by one, i.e., $0\leq b \leq B$ 
(note $\sum_{b=0}^{B}$ in the above expression).
%the summation from $0$ to $B$ in the above expression).

\para{Blocks and packets.}
Precisely, a \emph{packet} consists of bits from the same bit plane, from a \emph{block} of
negabinary wavelet coefficients. A block is a $[g\times g\times g]$ grid of adjacent coefficients
from the same subband. We let $g$ be a constant ($g=2$ in this paper), so that finer resolution
subbands contain more packets, which presents a trade-off between packets that provide wider (but
coarser) coverage and packets that provide finer (but more local) details. Every packet (of size one
byte in this paper) comes from a bit plane $b$ and a subband $l$. $g$ is chosen to be larger than
one for performance reasons, as in practice, most systems read bits in batches.

\subsection{Data-Dependent and Data-Independent Streams} \label{sec:static-dynamic-streams}

We define two streams: \emph{by level} and \emph{by bit plane}, which model two common reduction
schemes in the literature. The \emph{by level} stream, \slvl, orders the packets strictly from
coarser to finer subbands. Within the same subband, packets follow the row-major order of blocks and
then bit plane order (from 0 to $B$) within each coefficient. All bits for each coefficient are
streamed together. The other common ordering, \emph{by bit plane}, or \sbit, proceeds strictly from
higher ordered to lower ordered bit planes. Within the same bit plane, packets follow the subband
order (from $0$ to $L-1$) and then row-major order in each subband. \slvl and \sbit are designed to
mimic the way data is accessed in traditional methods that work either in resolution (\slvl) or in
precision (\sbit).

Additionally, we define a third stream that combines these two dimensions and refer to it as
\emph{by wavelet norm}, or \swav. This stream orders packets in descending order of weights
%$\wwav(p)=2^{B-b(p)}\times \norm{\wbas_{l(p)}}^2$,
$\wwav(p)=2^{B-b(p)}\times \| \wbas_{l(p)} \|$, where $p$ denotes a packet and $\wbas_{l(p)}$
represents wavelet basis on subband $l(p)$. The $||$ notation refers to the $L_2$ norm of the
wavelet basis function. The first term captures the contribution of a bit on bit plane $b(p)$, and
the second term captures the contribution of a wavelet coefficient on subband $l(p)$.
%
% \todo{
% Why the *squared* norm?  This is *not* the contribution of a bit to the error in the function.
% For a wavelet basis function w and associated coefficient $sum c(i) (-2)^i$, bit i (when equal to one) adds a contribution $w (-2)^i$ to the signal.  If you leave this bit out, you'll make an L2 error of $sqrt(||w||^2 (2^i)^2) = ||w|| 2^i$.
% Itâ€™s obviously too late to redo the results.  We could perhaps state the correct equation and then fix our results for the revision.  Or we could hope the reviewers find the flaw, and we'll fix it.  Otherwise we have to revise the paper with no reviewer pointing out the flaw.}
%
In the wavelet representation, a function $f$ is written as a linear sum of wavelet basis functions,
i.e., $f=\sum{c_i\wbas_i}$, where $c_i$ are the coefficients. Since our wavelet transforms are based
on lifting, this norm is usually not one, but it increases with level. Basis functions in the same
subband share the same norm, hence $\wwav(p)$ is simply the contribution (in $L_2$ norm) of a bit on
bit plane $b(p)$ and subband $l(p)$, to the whole function $f$. This ordering based on norms of
wavelet basis functions was proposed previously by Weiss \etal~\cite{weiss}. For details of
computing the norms of basis functions, see Appendix A.

Another common way to reduce data in the wavelet domain is to leave out the coefficients of the
smallest magnitudes. Note that coefficient magnitudes are only weakly related to error, as the error
also depends on the wavelet basis function norm~\cite{weiss}. We model this scheme with a stream
called \emph{by magnitude}, or \smag. Here, the weight function is $\wmag(p)=\sum_{c\in
\text{block}(p)}\norm{c}$ (the sum is over all coefficients in the block that contains packet $p$).
If two packets have the same weight, they are ordered by subband index and then by bit plane.

Unlike \slvl, \sbit, and \swav, the \smag stream is data dependent because the coefficient
magnitudes are not known without the data.
%
In principle, data-dependent streams are better than data-independent streams because they can
prioritize important packets based on the actual data. However, data-dependent streams are
ill-suited for practical purposes, because the cost of sending position information likely outweighs
any potential benefit. Nevertheless, we study them for various reasons. First, the \emph{by
magnitude} scheme is well known in the literature~\cite{vapor2007}. Second, the ``best''
data-dependent streams (which do not include position information for packets) can serve as a
baseline to evaluate the performance of their data-independent counterparts. Finally, in addition to
being data dependent, streams can also be task dependent (\Cref{sec:data_dep_streams}), which may
provide insights into how data should be queried to perform certain analysis tasks.

\subsection{Task-Optimized Streams} \label{sec:data_dep_streams}

Each analysis task may require a fundamentally different stream for optimal results. Studying such
``optimal'' streams is important because they not only serve as a baseline but also can provide
insights into other, more practical streams. Given the original data set $f$ and its reconstructed
approximation $f'$ using a subset of packets, let $q$ represent some quantity of interest, e.g.,
histogram, isosurface, etc., computed on $f$ or $f'$. For a given $q$, a well-defined error metric
$\err(q(f'),q(f))$, which returns a single scalar, is needed. Given $f$, $q$, and $\err$, our goal
is to generate an \emph{optimal} (and data-dependent) stream, $\sopt$, for $q$ with respect to
$\err$. One possible definition for $\sopt$ is a stream such that the area under the plotted curve
of \err, with respect to the number of bits, is minimized for all packets to be streamed. However,
this definition is limited in practice because a stream should be able to terminate at any point and
still produce as small an error as possible.

Instead, we employ a greedy approach to define the optimal stream. We notice through experiments,
however, that a straightforward greedy algorithm can pick unimportant packets too early. For
example, starting with an all-zero reconstruction $f'=0$ and an empty stream, we can repeatedly
append new packets to the stream, which when included in the current $f'$, would minimize $\err$ at
every step. At some point, the algorithm might pick a packet that introduces the lowest error yet
contributes very little to improve the quality of $f'$ (because more important packets would
increase the error), leading to a nonoptimal stream. To avoid this problem, we make a modification
to this greedy algorithm and build the stream backwards. We start with a ``lossless'' $f'$ (i.e.,
$f'=f$), and at each step we remove the packet that has the least impact on the error $\err$ from
$f'$. This modification largely avoids the previous problem where less important packets were added
to \sopt too early, because by starting with the full (instead of empty) set of packets, our error
measurement better captures the importance of packets.

Unfortunately, such a greedy algorithm is still expensive in practice, as its complexity is at least
$\mathcal{O}(n^2)$ ($n$ is the number of packets), due to the 2-level nested loop. For a $nx\times
ny \times nz$ volume, a block size of $bx\times by\times bz$, and $B+1$ bit planes, $n$ is
$\frac{nx}{bx}\times \frac{ny}{by}\times \frac{nz}{bz} \times (B+1)$. Thus, even a small volume,
e.g., $nx,ny,nz=64$ and $bx,by,bz=2$, can result in a prohibitively high run time, as
$n^2=(32768\times 17)^2$. Therefore, we adopt a simplified version of this algorithm, where only one
pass through $n$ packets is needed. In iteration $i$ ($0\leq i < n$), we set a new packet $p_i$ to
zero, compute and record the incurred error $w_i$ using the error metric $\err$, and then enable
$p_i$ again at the end of iteration $i$. After $n$ iterations, each packet has an associated weight
$w_i$. The stream \sopt is simply the sorted list of packets in decreasing order of the weights.
This simplified algorithm (\Cref{alg:greedy}) has significantly lower running time, while (by
observation) retaining the same quality for \sopt.

\vspace{-0.5em}
\begin{algorithm}[h]
  \caption{Computing a task-optimized stream}
  \begin{algorithmic}[1]
    \Inputs{
			An original function $f$\\
			An unordered set of $n$ packets $P = \{p_i\}$, produced from $f$\\
			A quantity of interest $q$, and an error function $\err$}
		\Initialize{A set of $n$ weights $\{w_i\}$ }
		\For{each packet $p_i$}
			\State $p_i := 0$
      \State $P \rightarrow$ wavelet coefficients $C=\{c_j\}$\\
      		\ \ \ \ \ \ \ \ \ \ \ \ (inverse quantization and inverse negabinary transform)
			\State $\{c_j\} \rightarrow f'$ (inverse wavelet transform)
			\State $w_i := \err(q(f'),q(f))$			
			\State Restore $p_i$
		\EndFor
		\State Sort the $p_i$'s in descending order of $w_i$.
		\Output{The $q$-optimized stream, which is the sorted $P$}
	\end{algorithmic}
	\label{alg:greedy}
\end{algorithm}

\vspace{-0.5em}
For a more optimized implementation, the inverse wavelet transform on line 7 can be replaced by
``splatting'' coefficient $p_i$ onto the domain, due to the transform being linear and the fact that
$p_i$ is the only coefficient changed in the current iteration. Since tt is almost never
advantageous to stream bits belonging to one coefficient out-of-order, we also enforce that in the
final stream, packets belonging to the same group follow the bit plane order (from $0$ to $B$).

\subsection{Stream Signatures} \label{sec:stream-signature}

Unlike data-independent streams, data-dependent streams do not impose a static ordering of packets.
To concisely represent and characterize the dynamic ordering of data-dependent streams, we introduce
the notion of a \emph{stream signature}. Any stream can be represented with respect to the
two-dimensional space of resolution (subbands) and precision (bit planes), i.e.,
\mbox{$\Lspace_{L,B}=\{(l,b)\ |\ 0\leq l < L,\ 0\leq b \leq B\}$.} Given a stream, we define its
signature $A$ as an $L \times (1+B)$ matrix, where each $(l,b)$ element is associated with
$P_{l,b}$, the set of packets belonging to subband $l$ and bit plane $b$. In particular, $A(l,b)$,
i.e., the $(l,b)$ element of $A$, is an integer in the range $[0, (1+B)\times L)$, and indicates, on
average, the position at which packets in $P_{l,b}$ appear in the given stream. For example, the
signature $A=\bigl[ \begin{smallmatrix}0 & 1 & 4\\
2 & 3 & 5\end{smallmatrix}\bigr]$ indicates that the stream begins with packets that lie on the
first bit plane of the first subband, as $A(0,0)=0$. Those are followed by packets on the second
bit plane of the first subband ($A(0,1)=1$), and then the first bit plane of the second subband
($A(1,0)=2$), and finally, the third bit plane of the second subband ($A(1,2)=5$). Thus, a
stream's signature shows how the stream traverses the space $\Lspace_{L,B}$ and highlights the
different resolution-versus-precision trade-offs among streams, especially among \sopt streams
optimized for different tasks. In~\Cref{fig:example-signatures} we visualize the signatures of
\sbit, \slvl, and \swav, defined in~\Cref{sec:data-streaming-framework}.

\begin{figure}[!t]
\centering
 \subcaptionbox{\emph{by level} (\slvl)}{{\includegraphics[width=0.32\linewidth]{SIG-(STATIC)-(BY-LEVEL)-(rmse)}\vspace{-0.5em}}}
 \subcaptionbox{\emph{by bit plane} (\sbit)}{{\includegraphics[width=0.32\linewidth]{SIG-(STATIC)-(BY-BIT-PLANE)-(rmse)}\vspace{-0.5em}}}
 \subcaptionbox{\emph{by wavelet norm} (\swav)}{ {\includegraphics[width=0.32\linewidth]{SIG-(STATIC)-(BY-WAVELET-NORM)-(rmse)}\vspace{-0.5em}}}
\vspace{-0.5em}
\caption{Visualization of signatures for \slvl, \sbit, and \swav in 2D. Each signature is a
$10\times 17$ image, corresponding to $10$ subbands (in 2D) and $17$ bit planes. Each $(l,b)$
``cell'' contains a unique value from $0$ to $169$, indicating its ``priority'' in the stream, and
is mapped to a white--blue color scale. \slvl streams bits by resolution (from top to bottom), \sbit
streams by precision (from left to right), while \swav mixes precision and resolution.}
\label{fig:example-signatures}
\vspace{-1.75em}
\end{figure}

To compute a stream signature, we partition the whole domain (not individual subbands) into several
\emph{regions}, compute one signature per region, and average these local signatures. Partitioning
is used since it is only when packets are relatively well localized that their relative ordering in
the $\Lspace_{L,B}$ space becomes meaningful. For example, a packet at one corner of the domain may
be streamed before one at an opposite corner, but this fact contains no useful information. We
define a region to be the spatial volume that is covered by a packet in the coarsest subband.
Algorithm~\ref{alg:signature} lists the steps of our approach.

Finally, a signature can be used to construct a stream denoted generically as $\ssig$. This
construction is done by iterating through each element $A(l,b)$ in ascending order and adding to the
end of \ssig all the packets in $P_{l,b}$. An \ssig captures the behavior (in the $\Lspace_{L,B}$
space) of the stream it derives from, but it is stripped from any spatial adaptivity. Hence, when
\ssig is derived from an \sopt, it can serve as a bridge when comparing the
resolution-versus-precision trade-offs between data-independent and data-dependent streams.

\vspace{-0.5em}
\begin{algorithm}[h]
  \caption{Computing a stream signature}
  \begin{algorithmic}[1]
    \Inputs{
			A stream $P=\{p_i\}$\\}
		\Initialize{Per-region signature matrix $A_r:= 0$\\
		Global signature matrix $A := 0$}
		\For{each packet $p_i$ in $P$}
			\State Let $r$, $b$, $l$ be the region, bit plane, and subband that $p_i$ belongs
			\State $A_r(l,b) := A_r(l,b)+i$
		\EndFor
		\For{each region $r$}
			\State Sort the elements of $A_r$
			\State Assign each element of $A_r$ its index after sorting
			\State $A := A+A_r$
		\EndFor
		\State Sort the elements of $A$
		\State Assign each element of $A$ its index after sorting
		\Output{The signature matrix $A$}
	\end{algorithmic}
	\label{alg:signature}
\end{algorithm}
\vspace{-0.5em}
