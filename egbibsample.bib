@article{hierarchical1984,
	author = {Samet, Hanan},
	title = {The Quadtree and Related Hierarchical Data Structures},
	journal = {ACM Comput. Surv.},
	issue_date = {June 1984},
	volume = {16},
	number = {2},
	month = jun,
	year = {1984},
	issn = {0360-0300},
	pages = {187--260},
	numpages = {74},
	url = {http://doi.acm.org/10.1145/356924.356930},
	doi = {10.1145/356924.356930},
	acmid = {356930},
	publisher = {ACM},
	address = {New York, NY, USA},
}


@article{amr1989,
	author = {Berger, M. J. and Colella, P.},
	title = {Local Adaptive Mesh Refinement for Shock Hydrodynamics},
	journal = {J. Comput. Phys.},
	issue_date = {May, 1989},
	volume = {82},
	number = {1},
	month = may,
	year = {1989},
	issn = {0021-9991},
	pages = {64--84},
	numpages = {21},
	url = {http://dx.doi.org/10.1016/0021-9991(89)90035-1},
	doi = {10.1016/0021-9991(89)90035-1},
	acmid = {1718335},
	publisher = {Academic Press Professional, Inc.},
	address = {San Diego, CA, USA},
} 


@book{compression_techniques1991,
	author = {Rabbani, Majid and Jones, Paul W.},
	title = {Digital Image Compression Techniques},
	year = {1991},
	isbn = {0819406481},
	edition = {1st},
	publisher = {Society of Photo-Optical Instrumentation Engineers (SPIE)},
	address = {Bellingham, WA, USA},
}


@article{histogram_intersection1991,
	author="Swain, Michael J.
	and Ballard, Dana H.",
	title="Color indexing",
	journal="International Journal of Computer Vision",
	year="1991",
	month="Nov",
	day="01",
	volume="7",
	number="1",
	pages="11--32",
	abstract="Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the location of a known object. Color can be successfully used for both tasks.",
	issn="1573-1405",
	doi="10.1007/BF00130487",
	url="https://doi.org/10.1007/BF00130487"
}


@article{image_compression1992,
	author = {R.A. DeVore and B. Jawerth and B.J. Lucier},
	title = {Image compression through wavelet transform coding},
	journal = {IEEE Trans. Inform. Theory},
	volume = {38},
	month = march,
	year = {1992},
	pages = {719--746},
} 


@inproceedings{vq1992,
	author = {Ning, Paul and Hesselink, Lambertus},
	title = {Vector Quantization for Volume Rendering},
	booktitle = {Proceedings of the 1992 Workshop on Volume Visualization},
	series = {VVS '92},
	year = {1992},
	isbn = {0-89791-527-5},
	location = {Boston, Massachusetts, USA},
	pages = {69--74},
	numpages = {6},
	url = {http://doi.acm.org/10.1145/147130.147152},
	doi = {10.1145/147130.147152},
	acmid = {147152},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


@ARTICLE{spiht1996,
	author={A. Said and W. A. Pearlman},
	journal={IEEE Transactions on Circuits and Systems for Video Technology},
	title={A new, fast, and efficient image codec based on set partitioning in hierarchical trees},
	year={1996},
	volume={6},
	number={3},
	pages={243-250},
	keywords={arithmetic codes;codecs;data compression;entropy codes;image coding;image reconstruction;transform coding;trees (mathematics);wavelet transforms;arithmetic code;decoding;decoding algorithm;embedded zerotree wavelet coding;entropy coding;file sizes;image codec;image coding;image compression;image reconstruction;image wavelet transform;ordered bit plane transmission;partial ordering;performance;self-similarity;set partitioning in hierarchical trees;set partitioning sorting algorithm;Codecs;Decoding;Image coding;Image reconstruction;Partitioning algorithms;Performance loss;Signal processing;Signal processing algorithms;Sorting;Wavelet transforms},
	doi={10.1109/76.499834},
	ISSN={1051-8215},
	month={Jun},
}


@INPROCEEDINGS{emd1998,
	author={Y. Rubner and C. Tomasi and L. J. Guibas},
	booktitle={Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)},
	title={A metric for distributions with applications to image databases},
	year={1998},
	volume={},
	number={},
	pages={59-66},
	keywords={image colour analysis;image texture;visual databases;color;distributions;easy-to-compute lower bounds;image databases;linear optimization;multi-dimensional scaling displays;partial matching;texture;transportation problem;Application software;Computer displays;Computer science;Frequency;Geoscience;Histograms;Image databases;Image retrieval;Navigation;Psychology},
	doi={10.1109/ICCV.1998.710701},
	ISSN={},
	month={Jan},
}


@inproceedings{multires_octree1999,
	author = {LaMar, Eric and Hamann, Bernd and Joy, Kenneth I.},
	title = {Multiresolution Techniques for Interactive Texture-based Volume Visualization},
	booktitle = {Proceedings of the Conference on Visualization '99: Celebrating Ten Years},
	series = {VIS '99},
	year = {1999},
	isbn = {0-7803-5897-X},
	location = {San Francisco, California, USA},
	pages = {355--361},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=319351.319432},
	acmid = {319432},
	publisher = {IEEE Computer Society Press},
	address = {Los Alamitos, CA, USA},
	keywords = {hardware texture, multiresolution rendering, volume visualization},
} 


@INPROCEEDINGS{sbhp2000,
	author={C. Chrysafis and A. Said and A. Drukarev and A. Islam and W. A. Pearlman},
	booktitle={2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)},
	title={SBHP-a low complexity wavelet coder},
	year={2000},
	volume={6},
	number={},
	pages={2035-2038 vol.4},
	keywords={code standards;computational complexity;data compression;entropy codes;image coding;transform coding;wavelet transforms;JPEG2000 image compression standard framework;SBHP;algorithm;compression performance;embedded coding;entropy coding;low complexity wavelet coder;low-complexity entropy coder;nonembedded coding;subband-block hierarchical partitioning;verification model;wavelet coefficients;Algorithm design and analysis;Code standards;Entropy coding;Image coding;Partitioning algorithms;Performance loss;Testing;Transform coding;Virtual manufacturing;Wavelet coefficients},
	doi={10.1109/ICASSP.2000.859233},
	ISSN={1520-6149},
	month={},
}


@ARTICLE{jpeg2001,
	author={A. Skodras and C. Christopoulos and T. Ebrahimi},
	journal={IEEE Signal Processing Magazine},
	title={The JPEG 2000 still image compression standard},
	year={2001},
	volume={18},
	number={5},
	pages={36-58},
	keywords={ISO standards;code standards;data compression;entropy codes;image coding;quantisation (signal);reviews;standardisation;telecommunication standards;transform coding;wavelet transforms;ISO/IEC;JPEG 2000 still image compression standard;Part I standard;entropy coding;error resilience;file format;image tiling;international standard;multicomponent transformations;performance comparisons;quantization;region-of-interest coding;scalability;standardization committee;visual weighting;wavelet transforms;Entropy coding;IEC standards;ISO standards;Image coding;Quantization;Resilience;Scalability;Standardization;Transform coding;Wavelet transforms},
	doi={10.1109/79.952804},
	ISSN={1053-5888},
	month={Sep},
}


@INPROCEEDINGS{idx2001,
	author={V. Pascucci and R. J. Frank},
	booktitle={Supercomputing, ACM/IEEE 2001 Conference},
	title={Global Static Indexing for Real-Time Exploration of Very Large Regular Grids},
	year={2001},
	volume={},
	number={},
	pages={45-45},
	keywords={Cache memory;Computer displays;Data visualization;Degradation;Grid computing;Indexing;Laboratories;Multiprocessing systems;Portable computers;Random access memory},
	doi={10.1145/582034.582036},
	ISSN={},
	month={Nov},
}


@INPROCEEDINGS{compression_domain2003,
	author={J. Schneider and R. Westermann},
	booktitle={IEEE Visualization, 2003. VIS 2003.},
	title={Compression domain volume rendering},
	year={2003},
	volume={},
	number={},
	pages={293-300},
	keywords={computer graphic equipment;image texture;rendering (computer graphics);vector quantisation;CPU;GPU;commodity graphics hardware;compression domain volume rendering;data transfer;gigabyte data sets;graphics processing unit;hierarchical quantization;multiresolution covariance analysis;programmable hardware;real-time rendering;temporal coherence;texture mapping hardware;texture memory;time-varying volumetric data set;vector quantization;Central Processing Unit;Computer graphics;Data visualization;Decoding;Encoding;Hardware;Large-scale systems;Rendering (computer graphics);Shock waves;Vector quantization},
	doi={10.1109/VISUAL.2003.1250385},
	ISSN={},
	month={Oct},
}

@inproceedings{multires_toolkit2003,
	author = {Clyne, John},
	pages = {152-157},
	title = {The multiresolution toolkit: Progressive access for regular gridded data},
	booktitle = {Proc. Visualization, Imaging, and Image Processing},
	year = {2003},
}


@INPROCEEDINGS{tf_decompression2004,
	author={P. Ljung and C. Lundstrom and A. Ynnerman and K. Museth},
	booktitle={2004 IEEE Symposium on Volume Visualization and Graphics},
	title={Transfer function based adaptive decompression for volume rendering of large medical data sets},
	year={2004},
	volume={},
	number={},
	pages={25-32},
	keywords={biomedical imaging;data compression;image coding;medical computing;rendering (computer graphics);transfer functions;wavelet transforms;compressed wavelet transformed blocks;image decompression;image quality;image rendering;medical imaging;medical transfer functions;multiresolution data representation;standard volumetric data sets;volume compression;volume rendering;Bandwidth;Biomedical imaging;Computed tomography;Data visualization;Image coding;Image resolution;Medical diagnostic imaging;Pipelines;Rendering (computer graphics);Transfer functions;Adaptive decompression;Image quality measures;Medical imaging;Multiresolution;Transfer function;Volume compression;Volume rendering;Wavelet transform},
	doi={10.1109/SVVG.2004.14},
	ISSN={},
	month={Oct},
}


@ARTICLE{speck2004,
	author={W. A. Pearlman and A. Islam and N. Nagaraj and A. Said},
	journal={IEEE Transactions on Circuits and Systems for Video Technology},
	title={Efficient, low-complexity image coding with a set-partitioning embedded block coder},
	year={2004},
	volume={14},
	number={11},
	pages={1219-1235},
	keywords={block codes;computational complexity;data compression;discrete cosine transforms;entropy codes;image coding;image colour analysis;optimisation;pattern clustering;transform coding;trees (mathematics);wavelet transforms;color image coding;discrete cosine transform;dyadic image transform;energy clustering;entropy coding;hierarchical trees;image wavelet transform coding algorithm;lossless coding;low-complexity image coding;rate distortion optimization;recursive set-partitioning embedded block coder;reversible coding;sorting mechanism;wavelet packets;Clustering algorithms;Color;Discrete cosine transforms;Discrete wavelet transforms;Frequency;Image coding;Partitioning algorithms;Transform coding;Wavelet coefficients;Wavelet packets;Color image coding;embedded coding;entropy coding;hierarchical coding;image coding;lossless coding;wavelet coding},
	doi={10.1109/TCSVT.2004.835150},
	ISSN={1051-8215},
	month={Nov},
}


@article{vapor2007,
	author={John Clyne and Pablo Mininni and Alan Norton and Mark Rast},
	title={Interactive desktop analysis of high resolution simulations: application to turbulent plume dynamics and current sheet formation},
	journal={New Journal of Physics},
	volume={9},
	number={8},
	pages={301},
	url={http://stacks.iop.org/1367-2630/9/i=8/a=301},
	year={2007},
}


@ARTICLE{hw_dvr2007,
	author={N. Fout and K. L. Ma},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	title={Transform Coding for Hardware-accelerated Volume Rendering},
	year={2007},
	volume={13},
	number={6},
	pages={1600-1607},
	keywords={computer graphic equipment;data compression;real-time systems;rendering (computer graphics);transform coding;GPU;compression quality;decompression;dequantization;graphics memory;hardware-accelerated volume rendering;inverse transform;large volume data sets;real-time volume rendering;software volume rendering;transform coding;volumetric compression;Data visualization;Decoding;Encoding;Graphics;Hardware;Pipelines;Production;Rendering (computer graphics);Resource management;Transform coding;Compressed Volume Rendering;Hardware-accelerated Volume Rendering;Transform Coding;Volume Compression},
	doi={10.1109/TVCG.2007.70516},
	ISSN={1077-2626},
	month={Nov},
}


@INPROCEEDINGS{woodring2011,
	author={J. Woodring and S. Mniszewski and C. Brislawn and D. DeMarle and J. Ahrens},
	booktitle={2011 IEEE Symposium on Large Data Analysis and Visualization},
	title={Revisiting wavelet compression for large-scale climate data using JPEG 2000 and ensuring data precision},
	year={2011},
	volume={},
	number={},
	pages={31-38},
	keywords={data analysis;data compression;data visualisation;electronic data interchange;environmental science computing;expert systems;production engineering computing;wavelet transforms;JPEG 2000;data analysis;data compression community;data precision;data quality;data transfer;data visualization;domain expert;large-scale climate data;large-scale data reading;large-scale data size;large-scale data writing;parallel ocean program data;production scientific computing;quality metric;remotely compressed POP data;signal processing;standard-based method;wavelet compression;Bit rate;Data visualization;Image coding;Quantization;Transform coding;Wavelet transforms},
	doi={10.1109/LDAV.2011.6092314},
	ISSN={},
	month={Oct},
}


@article {covra2012,
	author = {Gobbetti, Enrico and Iglesias Guitián, José Antonio and Marton, Fabio},
	title = {COVRA: A compression-domain output-sensitive volume rendering architecture based on a sparse representation of voxel blocks},
	journal = {Computer Graphics Forum},
	volume = {31},
	number = {3pt4},
	publisher = {Blackwell Publishing Ltd},
	issn = {1467-8659},
	url = {http://dx.doi.org/10.1111/j.1467-8659.2012.03124.x},
	doi = {10.1111/j.1467-8659.2012.03124.x},
	pages = {1315--1324},
	keywords = {Computer Graphics [I.3.3]: Picture/Image Generation—, Computer Graphics [I.3.7]: Three-dimensional graphics and realism—, Coding and Information Theory [E.4]: Data compaction and compression—, Compression (Coding) [I.4.2]: Approximate methods—},
	year = {2012},
}


@article{vdb2013,
	author = {Museth, Ken},
	title = {VDB: High-resolution Sparse Volumes with Dynamic Topology},
	journal = {ACM Trans. Graph.},
	issue_date = {June 2013},
	volume = {32},
	number = {3},
	month = jul,
	year = {2013},
	issn = {0730-0301},
	pages = {27:1--27:22},
	articleno = {27},
	numpages = {22},
	url = {http://doi.acm.org/10.1145/2487228.2487235},
	doi = {10.1145/2487228.2487235},
	acmid = {2487235},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {Volumes, fluid animation, implicit surfaces, level sets},
}


@inproceedings{compression_sim2013,
	author    = {Daniel E. Laney and
			   Steven Langer and
			   Christopher Weber and
			   Peter Lindstrom and
			   Al Wegener},
	title     = {Assessing the effects of data compression in simulations using physically
			   motivated metrics},
	booktitle = {International Conference for High Performance Computing, Networking,
			   Storage and Analysis, SC'13, Denver, CO, {USA} - November 17 - 21,
			   2013},
	pages     = {76:1--76:12},
	year      = {2013},
	url       = {http://doi.acm.org/10.1145/2503210.2503283},
	doi       = {10.1145/2503210.2503283},
	timestamp = {Mon, 05 Jun 2017 12:39:52 +0200},
	biburl    = {http://dblp.org/rec/bib/conf/sc/LaneyLWLW13},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{spgrid2014,
	author = {Setaluri, Rajsekhar and Aanjaneya, Mridul and Bauer, Sean and Sifakis, Eftychios},
	title = {SPGrid: A Sparse Paged Grid Structure Applied to Adaptive Smoke Simulation},
	journal = {ACM Trans. Graph.},
	issue_date = {November 2014},
	volume = {33},
	number = {6},
	month = nov,
	year = {2014},
	issn = {0730-0301},
	pages = {205:1--205:12},
	articleno = {205},
	numpages = {12},
	url = {http://doi.acm.org/10.1145/2661229.2661269},
	doi = {10.1145/2661229.2661269},
	acmid = {2661269},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {adaptive discretizations, fluid simulation, sparse grids},
}


@ARTICLE{zfp2014,
	author={P. Lindstrom},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	title={Fixed-Rate Compressed Floating-Point Arrays},
	year={2014},
	volume={20},
	number={12},
	pages={2674-2683},
	keywords={computer graphics;data compression;data visualisation;embedded systems;floating point arithmetic;graphics processing units;random-access storage;storage management;bit rate selection;data access;data visualization;embedded coding;fixed-point arithmetic operations;fixed-precision values;fixed-rate compressed floating-point arrays;fixed-rate texture compression methods;floating-point data;graphics hardware;hardware implementation;lossy compression;memory management;near-lossless compression scheme;numerical simulation;orthogonal block transform;per-block bit stream;quantitative data analysis;read and write random access;scientific applications;software write-back cache;variable-length bit stream;Bandwidth allocation;Computational modeling;Data visualization;Encoding;Floating-point arithmetic;Image coding;Data compression;embedded coding;floating-point arrays;orthogonal block transform},
	doi={10.1109/TVCG.2014.2346458},
	ISSN={1077-2626},
	month={Dec},
}


@article{tensor_dvr2015,
	author = {Ballester-Ripoll, Rafael and Suter, Susanne K. and Pajarola, Renato},
	title = {Analysis of Tensor Approximation for Compression-domain Volume Visualization},
	journal = {Comput. Graph.},
	issue_date = {April 2015},
	volume = {47},
	number = {C},
	month = apr,
	year = {2015},
	issn = {0097-8493},
	pages = {34--47},
	numpages = {14},
	url = {http://dx.doi.org/10.1016/j.cag.2014.10.002},
	doi = {10.1016/j.cag.2014.10.002},
	acmid = {2902848},
	publisher = {Pergamon Press, Inc.},
	address = {Elmsford, NY, USA},
	keywords = {Canonical decomposition, Higher-order decompositions, Tensor approximation, Tensor rank truncation, Tucker decomposition, Volume visualization},
}

@inproceedings{tamresh,
	author = {Suter, S. K. and Makhynia, M. and Pajarola, R.},
	title = {TAMRESH - Tensor Approximation Multiresolution Hierarchy for Interactive Volume Visualization},
	booktitle = {Proceedings of the 15th Eurographics Conference on Visualization},
	series = {EuroVis '13},
	year = {2013},
	location = {Leipzig, Germany},
	pages = {151--160},
	numpages = {10},
	url = {http://dx.doi.org/10.1111/cgf.12102},
	doi = {10.1111/cgf.12102},
	acmid = {2600556},
	publisher = {The Eurographs Association \&\#38; John Wiley \&\#38; Sons, Ltd.},
	address = {Chichester, UK},
} 

@article{fpzip,
	author = {Lindstrom, Peter and Isenburg, Martin},
	title = {Fast and Efficient Compression of Floating-Point Data},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	issue_date = {September 2006},
	volume = {12},
	number = {5},
	month = sep,
	year = {2006},
	issn = {1077-2626},
	pages = {1245--1250},
	numpages = {6},
	url = {http://dx.doi.org/10.1109/TVCG.2006.143},
	doi = {10.1109/TVCG.2006.143},
	acmid = {1187859},
	publisher = {IEEE Educational Activities Department},
	address = {Piscataway, NJ, USA},
	keywords = {High throughput, High throughput, lossless compression, file compaction for I/O efficiency, fast entropy coding, range coder, predictive coding, large scale simulation and visualization., fast entropy coding, file compaction for I/O efficiency, large scale simulation and visualization., lossless compression, predictive coding, range coder},
} 

@INPROCEEDINGS{sz, 
	author={D. Tao and S. Di and Z. Chen and F. Cappello}, 
	booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
	title={Significantly Improving Lossy Compression for Scientific Data Sets Based on Multidimensional Prediction and Error-Controlled Quantization}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={1129-1139}, 
	keywords={data compression;parallel processing;HPC applications;adaptive error-controlled quantization encoder;error-controlled lossy compression algorithm;multidimensional prediction;multilayer prediction formulas;normalized root mean squared error;scientific data sets;Adaptation models;Compression algorithms;Data models;Encoding;Measurement;Predictive models;Quantization (signal)}, 
	doi={10.1109/IPDPS.2017.115}, 
	ISSN={}, 
	month={May},
}

@INPROCEEDINGS{hvg, 
	author={J. Schneider and R. Westermann}, 
	booktitle={IEEE Visualization, 2003. VIS 2003.}, 
	title={Compression domain volume rendering}, 
	year={2003}, 
	volume={}, 
	number={}, 
	pages={293-300}, 
	keywords={computer graphic equipment;image texture;rendering (computer graphics);vector quantisation;CPU;GPU;commodity graphics hardware;compression domain volume rendering;data transfer;gigabyte data sets;graphics processing unit;hierarchical quantization;multiresolution covariance analysis;programmable hardware;real-time rendering;temporal coherence;texture mapping hardware;texture memory;time-varying volumetric data set;vector quantization;Central Processing Unit;Computer graphics;Data visualization;Decoding;Encoding;Hardware;Large-scale systems;Rendering (computer graphics);Shock waves;Vector quantization}, 
	doi={10.1109/VISUAL.2003.1250385}, 
	ISSN={}, 
	month={Oct},
}

@InProceedings{isabela,
	author="Lakshminarasimhan, Sriram
	and Shah, Neil
	and Ethier, Stephane
	and Klasky, Scott
	and Latham, Rob
	and Ross, Rob
	and Samatova, Nagiza F.",
	editor="Jeannot, Emmanuel
	and Namyst, Raymond
	and Roman, Jean",
	title="Compressing the Incompressible with ISABELA: In-situ Reduction of Spatio-temporal Data",
	booktitle="Euro-Par 2011 Parallel Processing",
	year="2011",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="366--379",
	abstract="Modern large-scale scientific simulations running on HPC systems generate data in the order of terabytes during a single run. To lessen the I/O load during a simulation run, scientists are forced to capture data infrequently, thereby making data collection an inherently lossy process. Yet, lossless compression techniques are hardly suitable for scientific data due to its inherently random nature; for the applications used here, they offer less than 10{\%} compression rate. They also impose significant overhead during decompression, making them unsuitable for data analysis and visualization that require repeated data access.",
	isbn="978-3-642-23400-2"
}

@inproceedings{sqe,
	author = {Iverson, Jeremy and Kamath, Chandrika and Karypis, George},
	title = {Fast and Effective Lossy Compression Algorithms for Scientific Datasets},
	booktitle = {Proceedings of the 18th International Conference on Parallel Processing},
	series = {Euro-Par'12},
	year = {2012},
	isbn = {978-3-642-32819-0},
	location = {Rhodes Island, Greece},
	pages = {843--856},
	numpages = {14},
	url = {http://dx.doi.org/10.1007/978-3-642-32820-6_83},
	doi = {10.1007/978-3-642-32820-6_83},
	acmid = {2402519},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
} 

@InProceedings{codar,
	author="Foster, Ian
	and Ainsworth, Mark
	and Allen, Bryce
	and Bessac, Julie
	and Cappello, Franck
	and Choi, Jong Youl
	and Constantinescu, Emil
	and Davis, Philip E.
	and Di, Sheng
	and Di, Wendy
	and Guo, Hanqi
	and Klasky, Scott
	and Van Dam, Kerstin Kleese
	and Kurc, Tahsin
	and Liu, Qing
	and Malik, Abid
	and Mehta, Kshitij
	and Mueller, Klaus
	and Munson, Todd
	and Ostouchov, George
	and Parashar, Manish
	and Peterka, Tom
	and Pouchard, Line
	and Tao, Dingwen
	and Tugluk, Ozan
	and Wild, Stefan
	and Wolf, Matthew
	and Wozniak, Justin M.
	and Xu, Wei
	and Yoo, Shinjae",
	editor="Rivera, Francisco F.
	and Pena, Tom{\'a}s F.
	and Cabaleiro, Jos{\'e} C.",
	title="Computing Just What You Need: Online Data Analysis and Reduction at Extreme Scales",
	booktitle="Euro-Par 2017: Parallel Processing",
	year="2017",
	publisher="Springer International Publishing",
	address="Cham",
	pages="3--19",
	abstract="A growing disparity between supercomputer computation speeds and I/O rates makes it increasingly infeasible for applications to save all results for offline analysis. Instead, applications must analyze and reduce data online so as to output only those results needed to answer target scientific question(s). This change in focus complicates application and experiment design and introduces algorithmic, implementation, and programming model challenges that are unfamiliar to many scientists and that have major implications for the design of various elements of supercomputer systems. We review these challenges and describe methods and tools that we are developing to enable experimental exploration of algorithmic, software, and system design alternatives.",
	isbn="978-3-319-64203-1"
}



@article{li2018,
	title = "Data Reduction Techniques for Scientific Visualization and Data Analysis",
	url = "https://pdfs.semanticscholar.org/e555/db81c878deb01f0e8003bc90a30898e3d518.pdf",
	note = "CGF submission in review?"
}

@ARTICLE{laplacian-pyramid, 
	author={P. Burt and E. Adelson}, 
	journal={IEEE Transactions on Communications}, 
	title={The Laplacian Pyramid as a Compact Image Code}, 
	year={1983}, 
	volume={31}, 
	number={4}, 
	pages={532-540}, 
	keywords={Image coding;Data compression;Data structures;Entropy;Frequency;Image coding;Image sampling;Laplace equations;Low pass filters;Pixel;Shape}, 
	doi={10.1109/TCOM.1983.1095851}, 
	ISSN={0090-6778}, 
	month={Apr},
}

@ARTICLE{weiss, 
	author={K. Weiss and P. Lindstrom}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Adaptive Multilinear Tensor Product Wavelets}, 
	year={2016}, 
	volume={22}, 
	number={1}, 
	pages={985-994}, 
	keywords={data visualisation;interpolation;inverse transforms;mesh generation;octrees;piecewise linear techniques;quadtrees;rendering (computer graphics);splines (mathematics);tensors;adaptive mesh elements;adaptive multilinear tensor product wavelets;adaptive nonconforming quadtree mesh;adaptive regular refinement;crack-free quadtree mesh;direct volume rendering;foundational visualization techniques;globally continuous functions;inverse wavelet transform;isosurfacing;linear B-splines;multilinear cells;nonzero wavelet coefficients;octree mesh;piecewise multilinear interpolation;texture mapping;Interpolation;Octrees;Splines (mathematics);Tensile stress;Wavelet domain;Wavelet transforms;Multilinear interpolation;adaptive wavelets;continuous reconstruction;multiresolution models;octrees}, 
	doi={10.1109/TVCG.2015.2467412}, 
	ISSN={1077-2626}, 
	month={Jan},
}

@ARTICLE{ezw, 
	author={J. M. Shapiro}, 
	journal={IEEE Transactions on Signal Processing}, 
	title={Embedded image coding using zerotrees of wavelet coefficients}, 
	year={1993}, 
	volume={41}, 
	number={12}, 
	pages={3445-3462}, 
	keywords={binary sequences;codecs;data compression;image coding;trees (mathematics);video equipment;wavelet transforms;EZW;adaptive arithmetic coding;binary decisions sequence;bit stream;discrete wavelet transform;embedded zerotree wavelet algorithm;entropy-coded successive-approximation quantization;fully embedded code;hierarchical subband decomposition;image compression algorithm;self-similarity;target distortion metric;target rate;universal lossless data compression;wavelet coefficients;Bit rate;Compression algorithms;Decoding;Discrete wavelet transforms;Image coding;Quantization;Rate distortion theory;Streaming media;Testing;Wavelet coefficients}, 
	doi={10.1109/78.258085}, 
	ISSN={1053-587X}, 
	month={Dec},
}

@ARTICLE{treib, 
	author={M. Treib and K. Bürger and F. Reichl and C. Meneveau and A. Szalay and R. Westermann}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Turbulence Visualization at the Terascale on Desktop PCs}, 
	year={2012}, 
	volume={18}, 
	number={12}, 
	pages={2169-2177}, 
	keywords={computational fluid dynamics;data visualisation;encoding;entropy;graphics processing units;ray tracing;turbulence;GPU memory;GPU system;bandwidth capacities;brick-based volume ray-casting;compressed flow field representation;desktop computers;entropy encoding;feature-based turbulence visualization;interactive selection;large-scale turbulent motions;memory capacities;run-length;simultaneous visualization;small-scale turbulent motions;spatio-temporal resolution;terascale on desktop PCs;turbulence properties;turbulence small-scale structure;unsteady turbulence simulations;velocity gradient tensor;visually guided exploration;wavelet-based compression scheme;Data visualization;Encoding;Graphics processing unit;Memory management;Rendering (computer graphics);Tensile stress;Vectors;Visualization system and toolkit design;data compression;data streaming;vector fields;volume rendering}, 
	doi={10.1109/TVCG.2012.274}, 
	ISSN={1077-2626}, 
	month={Dec},
}

@PhdThesis\{gigavoxels,
	author       = "Crassin, Cyril",
	title        = "GigaVoxels: A Voxel-Based Rendering Pipeline For Efficient Exploration Of Large And Detailed Scenes",
	school       = "UNIVERSITE DE GRENOBLE",
	month        = "July",
	year         = "2011",
	note         = "English and web-optimized version.",
	url          = "http://maverick.inria.fr/Publications/2011/Cra11"
}

@article{large‐scale-volume,
	author = {Johanna Beyer and Markus Hadwiger and Hanspeter Pfister},
	title = {State‐of‐the‐Art in GPU‐Based Large‐Scale Volume Visualization},
	journal = {Computer Graphics Forum},
	volume = {34},
	number = {8},
	pages = {13-37},
	keywords = {STAR, volume rendering, ray‐guided, large data, GPU, I.3.6 Computer Graphics: Methodology and Techniques—I.3.3 Computer Graphics: Picture/Image Generation—Display algorithms},
	doi = {10.1111/cgf.12605},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12605},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12605},
	abstract = {Abstract This survey gives an overview of the current state of the art in GPU techniques for interactive large‐scale volume visualization. Modern techniques in this field have brought about a sea change in how interactive visualization and analysis of giga‐, tera‐ and petabytes of volume data can be enabled on GPUs. In addition to combining the parallel processing power of GPUs with out‐of‐core methods and data streaming, a major enabler for interactivity is making both the computational and the visualization effort proportional to the amount and resolution of data that is actually visible on screen, i.e. ‘output‐sensitive’ algorithms and system designs. This leads to recent output‐sensitive approaches that are ‘ray‐guided’, ‘visualization‐driven’ or ‘display‐aware’. In this survey, we focus on these characteristics and propose a new categorization of GPU‐based large‐scale volume visualization techniques based on the notions of actual output‐resolution visibility and the current working set of volume bricks—the current subset of data that is minimally required to produce an output image of the desired display resolution. Furthermore, we discuss the differences and similarities of different rendering and data traversal strategies in volume rendering by putting them into a common context—the notion of address translation. For our purposes here, we view parallel (distributed) visualization using clusters as an orthogonal set of techniques that we do not discuss in detail but that can be used in conjunction with what we present in this survey.}
}

@inproceedings{level-of-detail-volume-rendering,
	author = {Weiler, Manfred and Westermann, R\"{u}diger and Hansen, Chuck and Zimmermann, Kurt and Ertl, Thomas},
	title = {Level-of-detail Volume Rendering via 3D Textures},
	booktitle = {Proceedings of the 2000 IEEE Symposium on Volume Visualization},
	series = {VVS '00},
	year = {2000},
	isbn = {1-58113-308-1},
	location = {Salt Lake City, Utah, USA},
	pages = {7--13},
	numpages = {7},
	url = {http://doi.acm.org/10.1145/353888.353889},
	doi = {10.1145/353888.353889},
	acmid = {353889},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@INPROCEEDINGS{sph-octree, 
	author={F. Reichl and M. Treib and R. Westermann}, 
	booktitle={2013 IEEE International Conference on Big Data}, 
	title={Visualization of big SPH simulations via compressed octree grids}, 
	year={2013}, 
	volume={}, 
	number={}, 
	pages={71-78}, 
	keywords={data compression;data visualisation;octrees;ray tracing;rendering (computer graphics);wavelet transforms;GPU;Interactive visualization;big SPH simulation;compressed octree grids;grid resolution;high-quality visualization;multiresolution multiblock grid;particle distribution;rendering;spatial dimension;spatially continuous 3D field;view-aligned grid;volume ray-casting;wavelet-based compression;Discrete wavelet transforms;Graphics processing units;Instruction sets;Octrees;Rendering (computer graphics);Smoothing methods;Three-dimensional displays;SPH;data compression;volume rendering}, 
	doi={10.1109/BigData.2013.6691717}, 
	ISSN={}, 
	month={Oct},
}

@Article{Gobbetti2008,
	author="Gobbetti, Enrico
	and Marton, Fabio
	and Iglesias Guiti{\'a}n, Jos{\'e} Antonio",
	title="A single-pass GPU ray casting framework for interactive out-of-core rendering of massive volumetric datasets",
	journal="The Visual Computer",
	year="2008",
	month="Jul",
	day="01",
	volume="24",
	number="7",
	pages="797--806",
	abstract="We present an adaptive out-of-core technique for rendering massive scalar volumes employing single-pass GPU ray casting. The method is based on the decomposition of a volumetric dataset into small cubical bricks, which are then organized into an octree structure maintained out-of-core. The octree contains the original data at the leaves, and a filtered representation of children at inner nodes. At runtime an adaptive loader, executing on the CPU, updates a view and transfer function-dependent working set of bricks maintained on GPU memory by asynchronously fetching data from the out-of-core octree representation. At each frame, a compact indexing structure, which spatially organizes the current working set into an octree hierarchy, is encoded in a small texture. This data structure is then exploited by an efficient stackless ray casting algorithm, which computes the volume rendering integral by visiting non-empty bricks in front-to-back order and adapting sampling density to brick resolution. Block visibility information is fed back to the loader to avoid refinement and data loading of occluded zones. The resulting method is able to interactively explore multi-gigavoxel datasets on a desktop PC. ",
	issn="1432-2315",
	doi="10.1007/s00371-008-0261-9",
	url="https://doi.org/10.1007/s00371-008-0261-9"
}

@ARTICLE{visualization-driven, 
	author={M. Hadwiger and J. Beyer and W. K. Jeong and H. Pfister}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Interactive Volume Exploration of Petascale Microscopy Data Streams Using a Visualization-Driven Virtual Memory Approach}, 
	year={2012}, 
	volume={18}, 
	number={12}, 
	pages={2285-2294}, 
	keywords={data acquisition;data visualisation;electron microscopes;microscopy;virtual storage;2D microscope image tiles;3D blocks;3D multiresolution representation;anisotropic petascale volume;best-of-breed system;cache misses;continuous stream;data acquisition;decouples construction;high resolution electron microscopy image;interactive volume exploration;microscopes;multiresolution hierarchy;multiresolution virtual memory architecture;neuroscience;octree;petascale microscopy data streams;petascale volumes;ray-casting;real microscopy data;system design;visible volume data;visualization-driven virtual memory;volume ray casting;volume visualization system;Data visualization;Graphics processing unit;Image resolution;Microscopy;Neuroscience;Octrees;Rendering (computer graphics);Petascale volume exploration;high-resolution microscopy;high-throughput imaging;neuroscience;0}, 
	doi={10.1109/TVCG.2012.240}, 
	ISSN={1077-2626}, 
	month={Dec},
}

@inproceedings{multires-framework,
	author = {Westermann, R\"{u}diger},
	title = {A Multiresolution Framework for Volume Rendering},
	booktitle = {Proceedings of the 1994 Symposium on Volume Visualization},
	series = {VVS '94},
	year = {1994},
	isbn = {0-89791-741-3},
	location = {Tysons Corner, Virginia, USA},
	pages = {51--58},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/197938.197963},
	doi = {10.1145/197938.197963},
	acmid = {197963},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@ARTICLE{volume-wavelet, 
	author={S. Muraki}, 
	journal={IEEE Computer Graphics and Applications}, 
	title={Volume data and wavelet transforms}, 
	year={1993}, 
	volume={13}, 
	number={4}, 
	pages={50-56}, 
	keywords={image processing;wavelet transforms;1D functions;3D orthogonal wavelet transforms;MR;magnetic resonance;real volume data;Computed tomography;Computer graphics;Discrete wavelet transforms;Laboratories;Laplace equations;Multidimensional systems;Object recognition;Shape;Signal resolution;Wavelet transforms}, 
	doi={10.1109/38.219451}, 
	ISSN={0272-1716}, 
	month={July},
}

@article{compression-domain-volume-rendering,
	author = {L. Lippert and M.H. Gross and C. Kurmann},
	title = {Compression Domain Volume Rendering for Distributed Environments},
	journal = {Computer Graphics Forum},
	volume = {16},
	number = {3},
	pages = {C95-C107},
	keywords = {volume rendering, multiresolution, progressive compression, splatting, wavelets, networks, distributed applications},
	doi = {10.1111/1467-8659.00146},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.00146},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-8659.00146},
	abstract = {This paper describes a method for volume data compression and rendering which bases on wavelet splats. The underlying concept is especially designed for distributed and networked applications, where we assume a remote server to maintain large scale volume data sets, being inspected, browsed through and rendered interactively by a local client. Therefore, we encode the server’s volume data using a newly designed wavelet based volume compression method. A local client can render the volumes immediately from the compression domain by using wavelet footprints, a method proposed earlier. In addition, our setup features full progression, where the rendered image is refined progressively as data comes in. Furthermore, framerate constraints are considered by controlling the quality of the image both locally and globally depending on the current network bandwidth or computational capabilities of the client. As a very important aspect of our setup, the client does not need to provide storage for the volume data and can be implemented in terms of a network application. The underlying framework enables to exploit all advantageous properties of the wavelet transform and forms a basis for both sophisticated lossy compression and rendering. Although coming along with simple illumination and constant exponential decay, the rendering method is especially suited for fast interactive inspection of large data sets and can be supported easily by graphics hardware.}
}

@inproceedings{interactive-rendering-large-volume,
	author = {Guthe, Stefan and Wand, Michael and Gonser, Julius and Stra\sser, Wolfgang},
	title = {Interactive Rendering of Large Volume Data Sets},
	booktitle = {Proceedings of the Conference on Visualization '02},
	series = {VIS '02},
	year = {2002},
	isbn = {0-7803-7498-3},
	location = {Boston, Massachusetts},
	pages = {53--60},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=602099.602106},
	acmid = {602106},
	publisher = {IEEE Computer Society},
	address = {Washington, DC, USA},
	keywords = {compression algorithms, level of detail algorithms, scientific visualization, volume rendering, wavelets},
} 

@ARTICLE{multiscale-tensor, 
	author={S. K. Suter and J. A. Iglesias Guitian and F. Marton and M. Agus and A. Elsener and C. P. E. Zollikofer and M. Gopi and E. Gobbetti and R. Pajarola}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Interactive Multiscale Tensor Reconstruction for Multiresolution Volume Visualization}, 
	year={2011}, 
	volume={17}, 
	number={12}, 
	pages={2135-2143}, 
	keywords={computer graphic equipment;coprocessors;data visualisation;interactive systems;natural sciences computing;rendering (computer graphics);CUDA;GPU accelerated out-of-core multiresolution rendering framework;GPU accelerated tensor reconstruction;computational simulations;constrained memory footprint;data transfer bandwidth;gigabyte sized microtomographic volumes;hierarchical brick tensor decomposition approach;high resolution 3D imaging devices;interactive multiscale tensor reconstruction;interactive visual analysis;multiresolution volume visualization;out-of-core memory footprint;tensor approximation;Approximation methods;Graphics processing unit;Instruction sets;Quantization;Rendering (computer graphics);Tensile stress;GPU/CUDA;interactive volume visualization;multiresolution rendering.;multiscale;tensor reconstruction;Algorithms;Animals;Computer Graphics;Computer Simulation;Databases, Factual;Hominidae;Imaging, Three-Dimensional;Lizards;Models, Anatomic;Molar;X-Ray Microtomography}, 
	doi={10.1109/TVCG.2011.214}, 
	ISSN={1077-2626}, 
	month={Dec},
}

@inproceedings{evaluating-compression-climate,
	author = {Baker, Allison H. and Xu, Haiying and Dennis, John M. and Levy, Michael N. and Nychka, Doug and Mickelson, Sheri A. and Edwards, Jim and Vertenstein, Mariana and Wegener, Al},
	title = {A Methodology for Evaluating the Impact of Data Compression on Climate Simulation Data},
	booktitle = {Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing},
	series = {HPDC '14},
	year = {2014},
	isbn = {978-1-4503-2749-7},
	location = {Vancouver, BC, Canada},
	pages = {203--214},
	numpages = {12},
	url = {http://doi.acm.org/10.1145/2600212.2600217},
	doi = {10.1145/2600212.2600217},
	acmid = {2600217},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {data compression, high performance computing},
} 

@ARTICLE{verifiable-isosurface, 
	author={T. Etiene and C. Scheidegger and L. G. Nonato and R. M. Kirby and C. Silva}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Verifiable Visualization for Isosurface Extraction}, 
	year={2009}, 
	volume={15}, 
	number={6}, 
	pages={1227-1234}, 
	keywords={image coding;image representation;convergence rate;isosurface extraction codes;isosurface features;visual representations;Computational modeling;Convergence;Data mining;Data visualization;Heart;Isosurfaces;Mathematical model;Numerical simulation;Pipelines;Scientific computing;Isosurface Extraction;Marching Cubes;V&V;Verification}, 
	doi={10.1109/TVCG.2009.194}, 
	ISSN={1077-2626}, 
	month={Nov},
}

@article{survey-multires,
	title = "Survey and analysis of multiresolution methods for turbulence data",
	journal = "Computers & Fluids",
	volume = "125",
	pages = "39 - 58",
	year = "2016",
	issn = "0045-7930",
	doi = "https://doi.org/10.1016/j.compfluid.2015.11.001",
	url = "http://www.sciencedirect.com/science/article/pii/S004579301500362X",
	author = "Jesus Pulido and Daniel Livescu and Jonathan Woodring and James Ahrens and Bernd Hamann",
	keywords = "Turbulence, Wavelet, Curvelet, Surfacelet, B-spline wavelet"
}

@ARTICLE{verifying-volume-rendering, 
	author={T. Etiene and D. Jönsson and T. Ropinski and C. Scheidegger and J. L. D. Comba and L. G. Nonato and R. M. Kirby and A. Ynnerman and C. T. Silva}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Verifying Volume Rendering Using Discretization Error Analysis}, 
	year={2014}, 
	volume={20}, 
	number={1}, 
	pages={140-154}, 
	keywords={data visualisation;error analysis;rendering (computer graphics);DVR algorithms;Riemann summation;continuous model;convergence curves;discretization error analysis;expected approximation errors;grid size;pixel size;volume rendering correctness verification approach;volume rendering integral analysis;Error analysis;Rendering (computer graphics);Testing;Volume measurements;Discretization errors;testing;verifiable visualization;verification;volume rendering}, 
	doi={10.1109/TVCG.2013.90}, 
	ISSN={1077-2626}, 
	month={Jan},
}

@INPROCEEDINGS{evaluating-efficacy-wavelet, 
	author={S. Li and K. Gruchalla and K. Potter and J. Clyne and H. Childs}, 
	booktitle={2015 IEEE 5th Symposium on Large Data Analysis and Visualization (LDAV)}, 
	title={Evaluating the efficacy of wavelet configurations on turbulent-flow data}, 
	year={2015}, 
	volume={}, 
	number={}, 
	pages={81-89}, 
	keywords={data analysis;data compression;data visualisation;image coding;parallel machines;wavelet transforms;data compression;data visualization;image processing;scientific data analysis;simulation code;supercomputer;turbulent-flow visualization tool;wavelet configuration;Data compression;Data visualization;Image coding;Kernel;Wavelet analysis;Wavelet transforms}, 
	doi={10.1109/LDAV.2015.7348075}, 
	ISSN={}, 
	month={Oct},
}

@INPROCEEDINGS{image-based-in-tisu-vis, 
	author={J. Ahrens and S. Jourdain and P. OLeary and J. Patchett and D. H. Rogers and M. Petersen}, 
	booktitle={SC14: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
	title={An Image-Based Approach to Extreme Scale in Situ Visualization and Analysis}, 
	year={2014}, 
	volume={}, 
	number={}, 
	pages={424-434}, 
	keywords={data analysis;data visualisation;image processing;public domain software;software tools;extreme scale data analysis;extreme scale in situ visualization;extreme scale scientific simulations;interactive image-based approach;open source tools;Analytical models;Atmospheric modeling;Cameras;Computational modeling;Data models;Data visualization;Databases}, 
	doi={10.1109/SC.2014.40}, 
	ISSN={2167-4329}, 
	month={Nov},
}

@inproceedings{in-situ-sampling-particle,
	author = {Woodring, J. and Ahrens, J. and Figg, J. and Wendelberger, J. and Habib, S. and Heitmann, K.},
	title = {In-situ Sampling of a Large-scale Particle Simulation for Interactive Visualization and Analysis},
	booktitle = {Proceedings of the 13th Eurographics / IEEE - VGTC Conference on Visualization},
	series = {EuroVis'11},
	year = {2011},
	location = {Bergen, Norway},
	pages = {1151--1160},
	numpages = {10},
	url = {http://dx.doi.org/10.1111/j.1467-8659.2011.01964.x},
	doi = {10.1111/j.1467-8659.2011.01964.x},
	acmid = {2422022},
	publisher = {The Eurographs Association \&\#38; John Wiley \&\#38; Sons, Ltd.},
	address = {Chichester, UK},
} 

@article{rapid-compression-volume,
	author = {Ky Giang Nguyen and Dietmar Saupe},
	title = {Rapid High Quality Compression of Volume Data for Visualization},
	journal = {Computer Graphics Forum},
	volume = {20},
	number = {3},
	pages = {49-57},
	doi = {10.1111/1467-8659.00497},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.00497},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-8659.00497},
	abstract = {Volume data sets resulting from, e.g., computerized tomography (CT) or magnetic resonance (MR) imaging modalities require enormous storage capacity even at moderate resolution levels. Such large files may require compression for processing in CPU memory which, however, comes at the cost of decoding times and some loss in reconstruction quality with respect to the original data. For many typical volume visualization applications (rendering of volume slices, subvolumes of interest, or isosurfaces) only a part of the volume data needs to be decoded. Thus, efficient compression techniques are needed that provide random access and rapid decompression of arbitrary parts the volume data. We propose a technique which is block based and operates in the wavelet transformed domain. We report performance results which compare favorably with previously published methods yielding large reconstruction quality gains from about 6 to 12 dB in PSNR for a5123 ‐volume extracted from the Visible Human data set. In terms of compression our algorithm compressed the data 6 times as much as the previous state‐of‐the‐art block based coder for a given PSNR quality.}
}

@article{wavelet-compression-interactive-vis,
	author = {Insung Ihm and Sanghun Park},
	title = {Wavelet‐Based 3D Compression Scheme for Interactive Visualization of Very Large Volume Data},
	journal = {Computer Graphics Forum},
	volume = {18},
	number = {1},
	pages = {3-15},
	keywords = {very large volume data, wavelets, 3D compression, fast random access, Visible Human data, interactive visualization},
	doi = {10.1111/1467-8659.00298},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.00298},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-8659.00298},
	abstract = {Interactive visualization of very large volume data has been recognized as a task requiring great effort in a variety of science and engineering fields. In particular, such data usually places considerable demands on run‐time memory space. In this paper, we present an effective 3D compression scheme for interactive visualization of very large volume data, that exploits the power of wavelet theory. In designing our method, we have compromised between two important factors: high compression ratio and fast run‐time random access ability. Our experimental results on the Visual Human data sets show that our method achieves fairly good compression ratios. In addition, it minimizes the overhead caused during run‐time reconstruction of voxel values. This 3D compression scheme will be useful in developing many interactive visualization systems for huge volume data, especially when they are based on personal computers or workstations with limited memory.}
}

@INPROCEEDINGS{mloc, 
	author={Z. Gong and T. Rogers and J. Jenkins and H. Kolla and S. Ethier and J. Chen and R. Ross and S. Klasky and N. F. Samatova}, 
	booktitle={2012 41st International Conference on Parallel Processing}, 
	title={MLOC: Multi-level Layout Optimization Framework for Compressed Scientific Data Exploration with Heterogeneous Access Patterns}, 
	year={2012}, 
	volume={}, 
	number={}, 
	pages={239-248}, 
	keywords={data compression;data reduction;distributed databases;natural sciences computing;network operating systems;optimisation;query processing;sampling methods;spatiotemporal phenomena;storage management;I/O systems;MLOC;combinatorial access pattern space;compressed scientific data exploration;compressed scientific spatiotemporal data;compression-driven data reduction;data size issue;database management technologies;exploratory data-intensive analytics;extreme-scale datasets;heterogeneous access patterns;multifile data partitioning;multilevel architecture;multilevel layout optimization framework;multiple fine-grained data layout optimization kernels;multiresolution data extraction;multiresolution data sampling;parallel file systems;precision-driven data analytics;query driven multivariate spatiotemporal constraints;query-driven data exploration;runtime environments;scientific simulations;storage capability;Data compression;Data models;Kernel;Layout;Optimization;Organizations;Throughput}, 
	doi={10.1109/ICPP.2012.39}, 
	ISSN={0190-3918}, 
	month={Sept},
}

@InProceedings{Fogal:2013:RayGuided,
	author = {Thomas Fogal AND Alexander Schiewe AND Jens Kr{\"}uger},
	title = {{An Analysis of Scalable GPU-Based Ray-Guided Volume Rendering}},
	booktitle={Large Data Analysis and Visualization (LDAV), 2013 IEEE Symposium on},
	year = {2013},
	url = {http://www.sci.utah.edu/~tfogal/academic/ray-guided/Fogal-Analysis.pdf}
}

@inproceedings{fogal-kdtree,
	author = {Fogal, Thomas and Childs, Hank and Shankar, Siddharth and Kr\"{u}ger, Jens and Bergeron, R. Daniel and Hatcher, Philip},
	title = {Large Data Visualization on Distributed Memory multi-GPU Clusters},
	booktitle = {Proceedings of the Conference on High Performance Graphics},
	series = {HPG '10},
	year = {2010},
	location = {Saarbrucken, Germany},
	pages = {57--66},
	numpages = {10},
	url = {http://dl.acm.org/citation.cfm?id=1921479.1921489},
	acmid = {1921489},
	publisher = {Eurographics Association},
	address = {Aire-la-Ville, Switzerland, Switzerland},
} 

@article{multires-volume-rendering,
	title = "Advanced techniques for high-quality multi-resolution volume rendering",
	journal = "Computers & Graphics",
	volume = "28",
	number = "1",
	pages = "51 - 58",
	year = "2004",
	issn = "0097-8493",
	doi = "https://doi.org/10.1016/j.cag.2003.10.018",
	url = "http://www.sciencedirect.com/science/article/pii/S0097849303002292",
	author = "S. Guthe and W. Strasser",
	keywords = "Viewing algorithms, Data compaction and compression"
}

@INPROCEEDINGS{out-of-core-algorithms,
	author = {Cláudio Silva and Yi-jen Chiang and Wagner Corrêa and Jihad El-sana and Peter Lindstrom},
	title = {Out-of-core algorithms for scientific visualization and computer graphics},
	booktitle = {In Visualization’02 Course Notes},
	year = {2002}
}

@INPROCEEDINGS{interactive-exploration-ct-scans, 
	author={S. Prohaska and A. Hutanu and R. Kahler and H. C. Hege}, 
	booktitle={IEEE Visualization 2004}, 
	title={Interactive exploration of large remote micro-CT scans}, 
	year={2004}, 
	volume={}, 
	number={}, 
	pages={345-352}, 
	keywords={computerised tomography;data visualisation;grid computing;image resolution;medical image processing;GridFTP protocol;data processing pipeline;hierarchical volume tenderer;human vertebral body;image resolution;imaging technology;interactive visualization;remote microCT scans;Biomedical imaging;Bones;Data processing;Data visualization;Detectors;Humans;Image resolution;Information retrieval;Osteoporosis;Pipelines}, 
	doi={10.1109/VISUAL.2004.51}, 
	ISSN={}, 
	month={Oct},
}

@ARTICLE{topology-verification-isosurface, 
	author={T. Etiene and L. G. Nonato and C. Scheidegger and J. Tienry and T. J. Peters and V. Pascucci and R. M. Kirby and C. T. Silva}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Topology Verification for Isosurface Extraction}, 
	year={2012}, 
	volume={18}, 
	number={6}, 
	pages={952-965}, 
	keywords={computational geometry;data visualisation;topology;algorithmic implementations;digital topology;isosurface coding mistakes;isosurface extraction;stratified Morse theory;topological invariant verification;topology verification;verifiable visualization;Face;Interpolation;Isosurfaces;Level set;Manifolds;Software;Topology;Verifiable visualization;isosurface;topology.}, 
	doi={10.1109/TVCG.2011.109}, 
	ISSN={1077-2626}, 
	month={June},
}

@ARTICLE{statistical-volume-quality, 
	author={C. Wang and K. L. Ma}, 
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={A Statistical Approach to Volume Data Quality Assessment}, 
	year={2008}, 
	volume={14}, 
	number={3}, 
	pages={590-602}, 
	keywords={data analysis;quality management;statistical analysis;wavelet transforms;data analysis;feature functions;large-scale data sets;predefined distance functions;reduced-reference approach;statistical approach;volume data quality assessment;wavelet domain;Statistical computing;Volume visualization;Algorithms;Computer Graphics;Data Interpretation, Statistical;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity}, 
	doi={10.1109/TVCG.2007.70628}, 
	ISSN={1077-2626}, 
	month={May},
}

@article{state-of-the-art-compressed-volume,
	author = {Balsa Rodr\'{\i}guez, M. and Gobbetti, E. and Iglesias Guiti\'{a}n, J.A. and Makhinya, M. and Marton, F. and Pajarola, R. and Suter, S.K.},
	title = {State-of-the-Art in Compressed GPU-Based Direct Volume Rendering},
	journal = {Comput. Graph. Forum},
	issue_date = {September 2014},
	volume = {33},
	number = {6},
	month = sep,
	year = {2014},
	issn = {0167-7055},
	pages = {77--100},
	numpages = {24},
	url = {https://doi.org/10.1111/cgf.12280},
	doi = {10.1111/cgf.12280},
	acmid = {3071836},
	publisher = {The Eurographs Association \&\#38; John Wiley \&\#38; Sons, Ltd.},
	address = {Chichester, UK},
	keywords = {Computer Graphics [I.3.3]: Picture/Image Generation Computer Graphics [I.3.7]: Three-dimensional graphics and realism Coding and Information Theory [E.4]: Data compaction and compression Compression Coding [I.4.2]: Approximate methods, GPU, compression models, compression-domain direct volume rendering, decoding strategies, decompression architectures, large volume data visualization, level-of-detail representations, preprocessing and encoding, sampling grid layouts, time-varying volume data visualization},
} 