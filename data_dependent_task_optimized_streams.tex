\section{Data dependent task-optimized streams}
\label{sec:data_dep_streams}

This section aims to solve the problem of finding the `'best'' stream possible, given a data set,
and an error metric. One way to define the best stream could be the stream that incurs the minimum
error at every point. However, in trying to realize such a stream, our experience has been that such
a stream does not exist. Assume otherwise that the optimal stream exists, then by its definition, it
must be possible to construct it using the following greedy algorithm: start with a pool of all
chunks, pick the one that minimizes the error, remove it from the pool of chunks, and repeat until
the pool is empty. Running this algorithm, we have noticed that there can be a situation in which
the next chunk that minimizes the error is on a low-order bit plane of a very fine-scale
coefficient, which contributes little to the reconstructed function. The error is minimized because
it is kept approximately constant. In this case it is actually better to pick a chunk that increases
the error, but otherwise contributes a lot more to the reconstructed function. In optimization
terms, it is necessary to move in a direction that increases the error to avoid getting stuck in a
local minima. 

\subsection{Computation}
In~\Cref{sec:combining} we demonstrated that different tasks may need different streams. For
example, PSNR stream significantly underperforms the histogram stream when applied to the histogram
query~(\Cref{fig:histogram-comparison}). Unfortunately, there is no single definition of best stream
is for a given query. It could be a stream that exhibit small changes in the data or stream that
reaches the smallest error fastest. Moreover, it is common to stop the streaming in case the
quantity of interest is good enough or the data size reached the limit of the machine.

We define the best stream as a sequence of refinements that reach the minimum error at given
stopping bit budget. Alas, in interactive application we can only make assumptions what will be the
number of bits streamed before user decides to stop the streaming. For example, if a user drasticly
changes viewpoint in a volume rendering application, the stream starts almost from scratch. Taking
the lack of control over the stopping budget to the extreme, the best stream becomes the one which
minimizes the error at all bit budgets. However, as in any optimization problem we may reach local
minimum, as chunks that improve the error but have impact on later refinement will have lower
priority. \hb{can be shortened} The existence of local minima prevents this progressive stream to be
globally optimal. We can compute the optimal stream by searching the ordering space for one that is
optimal for the largest number of bit budgets. Unfortunately, finding such ordering is exponential
in number of chunks. Therefore, we focus on finding a greedy scheme that could be good
representative for the optimal stream. There are two primary directions along which we can greedily
search for sream: fine-to-coarse or coarse-to-fine.

\paragraph*{Fine-to-coarse greedy algorithm} utilizes full dataset to construct the stream.
Moreover, if we wanted to precompute stream order which could be utilized during query, we would
have access to whole data set and could compute this stream. The algorithm is in principle a reverse
of the previous approach \hb{perhaps the order was switched?}, the stream is constructed by starting with full data set and one by one
disabling the chunks. At each step, a chunk with smallest errorr impact is disabled (TODO: more
detail). This algorithm is still of greedy nature as it makes only locally optimal choice. The
running time of this algorithm is $O(n^3)$ as we start with $n$ chunks and at each streaming step
decrease the chunk count by one. The cube factor comes from the need to perform inverse wavelet
transform and compute the error for each chunk.

\begin{algorithm}
  \KwData{slice, unordered list of chunks}
  \KwResult{ordered list of chunks with decreasing error}
  orderedChunks = $\emptyset$\;

  \While{$|$chunks$| \ne 0$}{
   smallestChunk = front(chunks)\;
   \For{chunk $\in$ chunks}{
     sliceCopy = slice\;
     disableChunk(sliceCopy, chunk)\;
     error = computeError(sliceCopy, slice)\;
     \If{error $<$ error(smallestChunk)}{
       smallestChunk = chunk\;
     }
   }

   disableChunk(slice, smallestChunk)\;
   chunks = chunks $\setminus$ smallestChunk\;
   orderedChunks = orderedChunks $\cup \{$smallestChunk$\}$\;
  }
  \caption{Fine-to-coarse stream optimization algorithms. \hb{use symbols for unordered / ordered sets? this would be strict total ordering nor partial ordering, right? 
  depending upon that, $\cup$ may or may not maintain ordering.  
  what is the `slice' in input? }}
\end{algorithm}

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{img/figure4_new/rmse-miranda-viscosity}
  \includegraphics[width=0.48\linewidth]{img/figure4_new/histogram-miranda-viscosity}
  \caption{Comparison of fine-to-coarse and by wavelet norm streams on Miranda viscosity data set.
            On the left is stream optimized for PSNR (higher is better) and on the right for
            histogram (lower is better). Despite using larger block size for the greedy stream ($16
            \times 16$) due to performance reasons, it still significantly outperforms the by
            wavelet norm stream for both quantities.}
\end{figure}

\paragraph*{Coarse-to-fine greedy algorithm} starts with no data and the initial error is computed
with respect to the full data set. Then it takes a list of all chunks in the dataset, computes the
error as if the chunk was enabled, and picks the chunk with the highest absolute difference in the
error with respect to the current error.  We use absolute difference to avoid the case where the
error difference is zero or negative, which would result in a long stream of chunk that do not
decrease the error significantly. \ptb{I am not sure in understand the previous sentence} This
assumption reflects the expectation of more data meaning better result. Similarly to the
coarse-to-fine algorithm, the running time is still $O(n^3)$.

Since the fine-to-coarse greedy stream outperforms \hb{needs to be pointed out using the figures}
the coarse-to-fine stream we further investigate possible runtime optimizations. Surprisingly,
performing only the first round of chunk error calculation and then sorting those chunks by the
error closely matches the full greedy algorithm. This simple optimization reduces the time
complexity to $O(n^2)$ and thus makes it more practical. We use this greedy scheme throught our
evaluation named \emph{fully adpative} stream.

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{img/figure6/rmse-miranda-viscosity}
  \includegraphics[width=0.48\linewidth]{img/figure6/histogram-miranda-viscosity}
  \caption{The fully adaptive stream (fine-to-coarse with only initial sorting) closely
            follows the coarse-to-fine and fine-to-coarse streams both for PSNR and histogram.}
\end{figure}
