\section{Data dependent task-optimized streams}
\label{sec:data_dep_streams}

The previous section used fixed ordered streams, either by bit plane, level, or wavelet norm. We
will show that these streams are suboptimal, and will describe a greedy heuristic to obtain better stream
tailored to a task. 

\subsection{Motivation}
To test whether a stream that is suited to a certain task (function reconstruction, using PSNR as
the metric) also performs well in another (histogram computation), we compare the streams introduced
in the previous section using a histogram error metric that is the Earth Mover Distance (EMD)~\cite{emd1998}.
The EMD measures how far the histogram of the reconstructed data is from the histogram of
the original data. We use 256 histogram bins in all experiments. The results are shown in Figure
\ref{fig:histogram-motivation}. Compared to Figure \ref{fig:motivation}, the \emph{by level} stream
is removed and two other streams are added: \emph{resolution-adaptive, EMD-optimized} and
\emph{fully adaptive, EMD-optimized}. The two new streams are analogous to their RMSE counterparts
(i.e., the two data-dependent streams in Figure \ref{fig:motivation}, now renamed to
\emph{resolution-adaptive, RMSE-optimized} and \emph{fully adaptive, RMSE-optimized}), but are
computed by greedily minimizing EMD instead of RMSE with regards to the ground truth.

\begin{figure}
	\centering
	\subcaptionbox{Magnetic}
	{\includegraphics[width=0.44\linewidth]{img/motivation/magnetic-histogram-motivation.pdf}}
	\subcaptionbox{Boiler-O2}
	{\includegraphics[width=0.44\linewidth]{img/motivation/boiler-histogram-motivation.pdf}}
	\subcaptionbox{Flame-OH}
	{\includegraphics[width=0.44\linewidth]{img/motivation/kflame-oh-histogram-motivation.pdf}}
	\subcaptionbox{Miranda-diffusivity}
	{\includegraphics[width=0.44\linewidth]{img/motivation/diffusivity-histogram-motivation.pdf}}
	\caption{Histogram error comparison. Lower EMD is better. The streams are truncated	towards the
	end where the errors become negligibly small.}
	\label{fig:histogram-motivation}
\end{figure}

The \emph{by bit plane} and \emph{by wavelet norm} streams match one another closely in terms of
PSNR, but differ significantly in EMD. Furthermore, the \emph{fully adaptive, RMSE-optimized} stream
underperforms  \emph{fully adaptive, EMD-optimized}, indicating that recontructing an accurate
function and reconstructing an accurate histogram require two very different orderings of bits.
Figure \ref{fig:histogram-comparison} illustrates how a quantitative difference in EMD translates to
a visual difference in histogram.

\begin{figure}
	\centering
	\subcaptionbox{\emph{by bit plane}}
	{\includegraphics[width=0.24\linewidth]{img/motivation/histogram-by-bit-plane.png}}
	\subcaptionbox{\emph{by wavelet norm}}
	{\includegraphics[width=0.24\linewidth]{img/motivation/histogram-by-wavelet-norm.png}}
	\subcaptionbox{\emph{fully adaptive, RMSE-optimized}}
	{\includegraphics[width=0.24\linewidth]{img/motivation/histogram-rmse-optimized.png}}
	\subcaptionbox{\emph{fully adaptive, EMD-optimized}}
	{\includegraphics[width=0.24\linewidth]{img/motivation/histogram-emd-optimized.png}}
	\caption{Histogram comparison for the Magnetic data set at 0.16 bits per sample. The reconstructed
	histogram (blue) is blended with the ground truth histogram (yellow), so green is where the two
	overlap. Larger overlap (more green, less blue and yellow) is better.}
	\label{fig:histogram-comparison}
\end{figure}



\subsection{Computation}
In~\Cref{sec:combining} we demonstrated that different tasks may need different streams. For example,
PSNR stream significantly underperforms the histogram stream when applied to the histogram
query~(\Cref{fig:histogram-comparison}).
Unfortunately, there is no single definition of best stream is for a given query. It could be a stream that
exhibit small changes in the data or stream that reaches the smallest error fastest.
Moreover, it is common to stop the streaming in case the quantity of interest is good enough or the data size
reached the limit of the machine.

We define the best stream as
a sequence of refinements that reach the minimum error at given stopping bit budget. Alas, in interactive application
we can only make assumptions what will be the number of bits streamed before user decides to stop the streaming. For
example, if a user drasticly changes viewpoint in a volume rendering application, the stream starts almost from scratch.
Taking the lack of control over the stopping budget to the extreme, the best stream becomes the one which minimizes
the error at all bit budgets. However, as in any optimization problem we may reach local minimum, as
chunks that improve the error but have impact on later refinement will have lower priority. The existence of local
minima prevents this progressive stream to be globally optimal.
We can compute the optimal stream by searching the ordering space for one that is optimal for the largest number
of bit budgets. Unfortunately, finding such ordering is exponential in number of chunks. Therefore, we focus on
finding a greedy scheme that could be good representative for the optimal stream. There are two primary directions
along which we can greedily search for sream: fine-to-coarse or coarse-to-fine.

\paragraph*{Fine-to-coarse greedy algorithm} utilizes full dataset to construct the stream.
Moreover,
if we wanted to precompute stream order which could be utilized during query, we would have access to whole
data set and could compute this stream. The algorithm is in principle a reverse of the previous approach, the
stream is constructed by starting with full data set and one by one disabling the chunks. At each step, a chunk
with smallest errorr impact is disabled (TODO: more detail). This algorithm is still of greedy nature as it
makes only locally optimal choice.
The running time of this algorithm is $O(n^3)$ as we start with $n$ chunks
and at each streaming step decrease the chunk count by one. The cube factor comes from the need to perform inverse
wavelet transform and compute the error for each chunk.

\begin{algorithm}
  \KwData{slice, unordered list of chunks}
  \KwResult{ordered list of chunks with decreasing error}
  orderedChunks = $\emptyset$\;

  \While{$|$chunks$| \ne 0$}{
   smallestChunk = front(chunks)\;
   \For{chunk $\in$ chunks}{
     sliceCopy = slice\;
     disableChunk(sliceCopy, chunk)\;
     error = computeError(sliceCopy, slice)\;
     \If{error $<$ error(smallestChunk)}{
       smallestChunk = chunk\;
     }
   }

   disableChunk(slice, smallestChunk)\;
   chunks = chunks $\setminus$ smallestChunk\;
   orderedChunks = orderedChunks $\cup \{$smallestChunk$\}$\;
  }
  \caption{Fine-to-coarse stream optimization algorithms}
\end{algorithm}

\begin{figure}
        \centering
        \includegraphics[width=0.48\linewidth]{img/figure4_new/rmse-miranda-viscosity}
        \includegraphics[width=0.48\linewidth]{img/figure4_new/histogram-miranda-viscosity}
        \caption{Comparison of fine-to-coarse and by wavelet norm streams on Miranda viscosity data set.
                 On the left is stream optimized for PSNR (higher is better) and on the right for histogram (lower is better).
                 Despite using larger block size for the greedy stream ($16 \times 16$) due to performance reasons, it still
                 significantly outperforms the by wavelet norm stream for both quantities.}
\end{figure}


\paragraph*{Coarse-to-fine greedy algorithm} starts with no data and the initial error is computed with respect
to the full data set. Then it takes a list of all chunks in the dataset, computes the error as if the chunk was
enabled, and picks the chunk with the highest absolute difference in the error with respect to the current error.
We use absolute difference to avoid the case where the error difference is zero or negative, which would result
in a long stream of chunk that do not decrease the error significantly. This assumption reflects the expectation
of more data meaning better result. Similarly to the coarse-to-fine algorithm, the running time is still $O(n^3)$.


Since the fine-to-coarse greedy stream outperforms the coarse-to-fine stream we further investigate possible
runtime optimizations. Surprisingly, performing only the first round of chunk error calculation and then sorting
those chunks by the error closely matches the full greedy algorithm. This simple optimization reduces the time
complexity to $O(n^2)$ and thus makes it more practical. We use this greedy scheme throught our evaluation named
\emph{fully adpative} stream.

\begin{figure}
        \centering
        \includegraphics[width=0.48\linewidth]{img/figure6/rmse-miranda-viscosity}
        \includegraphics[width=0.48\linewidth]{img/figure6/histogram-miranda-viscosity}
        \caption{The fully adaptive stream (fine-to-coarse with only initial sorting) closely follows the coarse-to-fine and
                 fine-to-coarse streams both for PSNR and histogram.}
\end{figure}
